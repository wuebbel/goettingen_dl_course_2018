{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62407f0c-9266-4d3e-9ef5-b6313eb44e51",
   "metadata": {},
   "source": [
    "# Deep Learning with Python and Pytorch - a quick Introduction\n",
    "In this demo, we create a simplistic neural network, consisting of two fully connected linear layers and a ReLU layer, that classifies the 28x28 MNIST image dataset.\n",
    "\n",
    "For simplicity, we do the learning stuff by using PyTorch as a black box at the beginning, but then do all forward classification manually.\n",
    "\n",
    "We will later add a section on how exactly the learning is done.\n",
    "\n",
    "In this notebook, you learn:\n",
    "\n",
    "- How to define a simple network using torch\n",
    "- How to learn weights for the network using torch\n",
    "- How to test and apply the forward model using torch\n",
    "- How to extract the weights and parameters to numpy\n",
    "- How to run the forward model using your own code, giving you full control of the network\n",
    "- How to interpret input layers for image classification\n",
    "- Application: Use your forward code for individual pruning of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b56c5-2b49-4975-94d5-4bfda1c9f8d1",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e626b24a-bbeb-44e8-98e0-6e92eb339e3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split,DataLoader,TensorDataset\n",
    "from torchviz import make_dot\n",
    "import matplotlib.pyplot as plt\n",
    "import onnx\n",
    "import numpy as np\n",
    "from onnx import numpy_helper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fece93a-de3f-4949-91b4-259248fb0904",
   "metadata": {},
   "source": [
    "### Check for GPU\n",
    "We check if GPU is available.\n",
    "\n",
    "We only use the GPU for the learning step. Just before learning, we copy the model to the gpu and then copy back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e5aba-20d5-4bff-8507-3496f676e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available. If yes, use it for learning.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Not working properly for me\n",
    "# torch.set_default_device(device)\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f1996-a237-47e0-9c45-cc169c4eb1b8",
   "metadata": {},
   "source": [
    "## Training phase\n",
    "We follow up with the complete training phase, defining parameters, loading the training data, and performing the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190ab2f-0413-4689-827f-11b348bd6497",
   "metadata": {},
   "source": [
    "### Define parameters\n",
    "- Input size for MNIST is 28x28=784\n",
    "- We use 1 hidden layer with 128 neurons\n",
    "- We classify into 10 classes\n",
    "- We use a batch size of 100\n",
    "- We use 5 iterations of the ADAM minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a58249-ccde-4eb2-aac9-21230b28165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "input_size = 784  # 28x28 Bilder\n",
    "hidden_size = 128\n",
    "output_size = 10  # 10 Klassen f√ºr die Ziffern 0-9\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "test_size = 10000 # Reserve test_size images for later testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0fec92-693d-458c-8bf8-909aa339c51f",
   "metadata": {},
   "source": [
    "### Define the neural network\n",
    "Our Network has two linear layers connected by a ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329a39a-669a-402b-b23c-d052efacadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einfaches neuronales Netzwerk definieren\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Bild in Vektor umwandeln\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70751d9-d1f4-427a-9147-8023437bf218",
   "metadata": {},
   "source": [
    "### Load MNIST dataset\n",
    "The MNIST dataset contains 60000 images.\n",
    "\n",
    "We train on a subset and reserve some images for later testing. Thus, we make sure that we do not test on images that were used for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03058363-3d0b-4321-b628-543726b030d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz laden und transformieren\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 1. Calculate the sizes for train/test subsets\n",
    "train_size = len(full_dataset) - test_size\n",
    "\n",
    "# 2. Use random_split to create two subsets\n",
    "train_subset, test_subset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# 3. Wrap them in DataLoaders if desired\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_subset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6991f3e-1509-4609-9e09-038014749b60",
   "metadata": {},
   "source": [
    "### Show some images from the dataset\n",
    "To get an idea of what the 60000 images look like, we display some random ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4301d6-cb13-41a4-b493-b067e7423102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some images\n",
    "num_images=4\n",
    "images, labels = next(iter(test_loader))  # Get a batch of test data\n",
    "# Convert images to numpy for visualization\n",
    "images = images.numpy()\n",
    "fig, axes = plt.subplots(1, num_images, figsize=(15, 4))\n",
    "for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i].squeeze(), cmap='gray')  # Display the image\n",
    "        ax.set_title(f'T: {labels[i].item()}',fontsize=30)  # Predictions and true labels\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0ef39-2cd3-4b49-9559-3f26654f1d20",
   "metadata": {},
   "source": [
    "### Perform training and miminization\n",
    "We use CrossEntropy for Loss and Adam as minimizer.\n",
    "\n",
    "Cross Entropy is often used for integer outputs and defined as\n",
    "$$\n",
    "\\ell(\\mathbf{x}, k)\n",
    "= -\\log \\Biggl( \\frac{\\exp(x_k)}{\\displaystyle \\sum_{j=1}^{N} \\exp(x_j)} \\Biggr)\n",
    "= -\\,x_k \\;+\\; \\log \\Biggl(\\sum_{j=1}^{N} \\exp(x_j)\\Biggr),\n",
    "$$\n",
    "\n",
    "In this, $k$ is the true value and $x$ is the output delivered by the network. For a batch of samples, this is averaged over all samples (or summed up, equivalently).\n",
    "\n",
    "The Adam-Minimizer is a weighted gradient descent minimization strategy, defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1)\\,g_t,\\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2)\\,g_t^2,\\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t}, \n",
    "\\quad\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t},\\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha \\,\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g_t &: \\text{gradient of the loss w.r.t. } \\theta_t,\\\\\n",
    "\\alpha &: \\text{learning rate},\\\\\n",
    "\\beta_1, \\beta_2 &: \\text{exponential decay rates for the moving averages of } m_t, v_t,\\\\\n",
    "\\varepsilon &: \\text{small constant for numerical stability}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The exact program is given in https://pytorch.org/docs/stable/generated/torch.optim.Adam.html. The initial values for all parameters are printed out. It seems that they are not changed in the run of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaade2b4-aa55-4899-af12-832a57a77b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verlustfunktion und Optimierer\n",
    "model=model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(optimizer)\n",
    "\n",
    "# Training des Modells\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Vorw√§rtsdurchlauf\n",
    "        outputs = model(images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        \n",
    "        # R√ºckw√§rtsdurchlauf und Optimierung\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Schritt [{i+1}/{len(train_loader)}], Verlust: {loss.item():.4f}')\n",
    "\n",
    "model=model.to('cpu')\n",
    "\n",
    "print(\"Training abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de06af9e-342d-4baf-9936-d33e28c7cd17",
   "metadata": {},
   "source": [
    "### Export the network\n",
    "We later want to analyze the network, export as ONNX and png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b28f4e-8dee-4ab2-b8c8-414ad431efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export network to model.onnx and model_architecture.png\n",
    "# Create a dummy input tensor with the appropriate shape\n",
    "dummy_input = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
    "\n",
    "# Perform a forward pass to obtain the model's output\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Generate the visualization\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "# Save the diagram to a file\n",
    "dot.format = 'png'\n",
    "dot.render('model_architecture')\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 28, 28)  # Adjust dimensions as needed\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\")"
   ]
  },
  {
   "attachments": {
    "a3f57d71-b903-49d4-8150-227dae00e17c.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "359ebb16-2369-41aa-b11c-9e57fbe047d0",
   "metadata": {},
   "source": [
    "Load the ONNX file model.onnx into https://netron.app/ to receive this representation. Note that the ONNX contains all data of the network, including weights and initializers.\n",
    "\n",
    "![image.png](attachment:a3f57d71-b903-49d4-8150-227dae00e17c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c5379-7d63-4ea3-9c58-c72787c3f43f",
   "metadata": {},
   "source": [
    "### Some test images\n",
    "Now finally, some test images together with their predictions. P is Prediction, T is True. First row is from the training dataset, second row is from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c642406-0abc-41bd-a3da-359268160a0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to display a batch of images with predictions and true labels\n",
    "def show_predictions(model, test_loader, num_images=10):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    images, labels = next(iter(test_loader))  # Get a batch of test data\n",
    "    outputs = model(images)  # Get predictions\n",
    "    _, predictions = torch.max(outputs, 1)  # Extract the class with the highest score\n",
    "    \n",
    "    # Convert images to numpy for visualization\n",
    "    images = images.numpy()\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 4))\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i].squeeze(), cmap='gray')  # Display the image\n",
    "        ax.set_title(f'P: {predictions[i].item()} | T: {labels[i].item()}',fontsize=30)  # Predictions and true labels\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_predictions(model, train_loader, num_images=4)\n",
    "show_predictions(model, test_loader, num_images=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d20eaf-f3df-44c0-83bd-3ed1dd66f66d",
   "metadata": {},
   "source": [
    "## Manual forward model\n",
    "In order to get a better insight into the model, we now turn away from Pytorch. We load the ONNX model, extract the network parameters and run everything in plain.\n",
    "\n",
    "We start by loading the ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05eed03-9916-46ab-b910-d32dcabc4de1",
   "metadata": {},
   "source": [
    "### Load ONNX model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49720582-227e-4846-a384-fac761b7f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "def read_onnx_model(onnx_file):\n",
    "    # Load ONNX model\n",
    "    model = onnx.load(onnx_file)\n",
    "    onnx.checker.check_model(model)\n",
    "    print(f\"Model {onnx_file} loaded successfully.\")\n",
    "    return model\n",
    "def extract_model_info(onnx_model):\n",
    "    # Get graph from the model\n",
    "    graph = onnx_model.graph\n",
    "    layers = {node.name: node for node in graph.node}\n",
    "    input_tensors = {input.name: input for input in graph.input}\n",
    "    output_tensors = {output.name: output for output in graph.output}\n",
    "    initializers = {initializer.name: initializer for initializer in graph.initializer}\n",
    "    return layers, input_tensors, output_tensors, initializers\n",
    "\n",
    "def print_onnx_info(onnx_model):\n",
    "    # Load ONNX model\n",
    "    model = onnx_model\n",
    "    onnx.checker.check_model(model)\n",
    "    print(f\"Model {onnx_file} loaded successfully.\\n\")\n",
    "\n",
    "    # Get the graph\n",
    "    graph = model.graph\n",
    "\n",
    "    # Print inputs\n",
    "    print(\"== Model Inputs ==\")\n",
    "    for input_tensor in graph.input:\n",
    "        dims = [d.dim_value if d.dim_value > 0 else \"?\" for d in input_tensor.type.tensor_type.shape.dim]\n",
    "        print(f\"Name: {input_tensor.name}, Type: {input_tensor.type.tensor_type.elem_type}, Shape: {dims}\")\n",
    "\n",
    "    # Print outputs\n",
    "    print(\"\\n== Model Outputs ==\")\n",
    "    for output_tensor in graph.output:\n",
    "        dims = [d.dim_value if d.dim_value > 0 else \"?\" for d in output_tensor.type.tensor_type.shape.dim]\n",
    "        print(f\"Name: {output_tensor.name}, Type: {output_tensor.type.tensor_type.elem_type}, Shape: {dims}\")\n",
    "\n",
    "    # Print initializers\n",
    "    print(\"\\n== Model Initializers ==\")\n",
    "    for initializer in graph.initializer:\n",
    "        dims = initializer.dims\n",
    "        print(f\"Name: {initializer.name}, Type: {initializer.data_type}, Shape: {dims}\")\n",
    "\n",
    "    # Print nodes (operations)\n",
    "    print(\"\\n== Model Nodes ==\")\n",
    "    for node in graph.node:\n",
    "        print(f\"Operation: {node.op_type}, Name: {node.name}\")\n",
    "        print(f\"  Inputs: {', '.join(node.input)}\")\n",
    "        print(f\"  Outputs: {', '.join(node.output)}\")\n",
    "\n",
    "    # Print value info (additional tensor details)\n",
    "    print(\"\\n== Additional Tensor Info ==\")\n",
    "    for value_info in graph.value_info:\n",
    "        dims = [d.dim_value if d.dim_value > 0 else \"?\" for d in value_info.type.tensor_type.shape.dim]\n",
    "        print(f\"Name: {value_info.name}, Type: {value_info.type.tensor_type.elem_type}, Shape: {dims}\")\n",
    "\n",
    "onnx_file = \"model.onnx\"  # Replace with your ONNX file path\n",
    "onnx_model = read_onnx_model(onnx_file)\n",
    "layers, input_tensors, output_tensors, initializers = extract_model_info(onnx_model)\n",
    "print_onnx_info(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4889181-6fff-4fcb-a5f4-874aecac652e",
   "metadata": {},
   "source": [
    "## Modeling the prediction step\n",
    "Our simple model has only two types of network nodes. Relu and Gemm. \n",
    "\n",
    "The ReLU transfer function is simply defined as\n",
    "$$\n",
    "\\operatorname{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "where $x$ is a vector and the max is taken componentwise.\n",
    "\n",
    "For Gemm, we have $$\n",
    "\\mathbf{Y} \n",
    "= \\alpha \\Bigl(\n",
    "  \\underbrace{\n",
    "    \\begin{cases}\n",
    "      \\mathbf{A}^\\top & \\text{if }\\text{transA} = 1,\\\\\n",
    "      \\mathbf{A}      & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  }_{\\tilde{\\mathbf{A}}}\n",
    "  \\;\\times\\;\n",
    "  \\underbrace{\n",
    "    \\begin{cases}\n",
    "      \\mathbf{B}^\\top & \\text{if }\\text{transB} = 1,\\\\\n",
    "      \\mathbf{B}      & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "  }_{\\tilde{\\mathbf{B}}}\n",
    "\\Bigr)\n",
    "\\;+\\;\n",
    "\\beta\\,\\mathbf{C},\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\tilde{\\mathbf{A}} \\in \\mathbb{R}^{m \\times k}, \n",
    "\\quad\n",
    "\\tilde{\\mathbf{B}} \\in \\mathbb{R}^{k \\times n}, \n",
    "\\quad\n",
    "\\mathbf{C} \\in \\mathbb{R}^{m \\times n},\n",
    "\\quad\n",
    "\\alpha,\\beta \\in \\mathbb{R},\n",
    "$$\n",
    "and \n",
    "$\\tilde{\\mathbf{A}}$ (resp. $\\tilde{\\mathbf{B}}$) denotes $\\mathbf{A}$ (resp. $\\mathbf{B}$) possibly transposed based on the attributes $\\text{transA}$ and $\\text{transB}$. The operator $\\times$ is standard matrix multiplication.\n",
    "\n",
    "That lets us define the forward model run. Note that this is not general, it is taylormade for our network, take are when applying it to networks of different structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222f568-9444-418a-af5e-981d2f992f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom run function\n",
    "def run_model(layers, initializers, input_data):\n",
    "    # Parse initializers into numpy arrays\n",
    "    weights = {}\n",
    "    for name, initializer in initializers.items():\n",
    "        weights[name] = np.frombuffer(initializer.raw_data, dtype=np.float32).reshape(tuple(initializer.dims))\n",
    "\n",
    "    # Simulate forward pass manually\n",
    "    data = input_data\n",
    "    for layer_name, layer in layers.items():\n",
    "        inputs = [data[input_name] if input_name in data else weights[input_name] for input_name in layer.input]\n",
    "        if layer.op_type == \"Gemm\":\n",
    "            # Fully connected layer (matrix multiplication + bias)\n",
    "            A, B, C = inputs  # A: input, B: weights, C: bias\n",
    "            alpha = float(next((attr.f for attr in layer.attribute if attr.name == \"alpha\"), 1.0))\n",
    "            beta = float(next((attr.f for attr in layer.attribute if attr.name == \"beta\"), 1.0))\n",
    "            transA = int(next((attr.i for attr in layer.attribute if attr.name == \"transA\"), 0))\n",
    "            transB = int(next((attr.i for attr in layer.attribute if attr.name == \"transB\"), 0))\n",
    "            \n",
    "            if transA:\n",
    "                A = A.T\n",
    "            if transB:\n",
    "                B = B.T\n",
    "            \n",
    "            output = alpha * np.dot(A, B) + beta * C\n",
    "            data[layer.output[0]] = output\n",
    "\n",
    "        elif layer.op_type == \"Relu\":\n",
    "            # Rectified Linear Unit activation\n",
    "            A = inputs[0]\n",
    "            output = np.maximum(0, A)\n",
    "            data[layer.output[0]] = output\n",
    "        \n",
    "        elif layer.op_type == \"Flatten\":\n",
    "            # Flatten the input tensor\n",
    "            A = inputs[0]\n",
    "            axis = int(next((attr.i for attr in layer.attribute if attr.name == \"axis\"), 1))\n",
    "            output = A.reshape(A.shape[0], -1)\n",
    "            data[layer.output[0]] = output\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Layer type {layer.op_type} not implemented.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5842790-7efb-477f-bf51-d4cb90b6c549",
   "metadata": {},
   "source": [
    "### Compare our forward run and Pytorch\n",
    "We check that our simple forward model and pytorch give the same results. Note that the strange name '8' for the output layer comes from ONNX and is given in the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ae92d-d8c6-4828-adfb-a70f2ee544d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# compare the models\n",
    "def compare_models(onnx_layers,onnx_initializers, pytorch_model, test_loader, num_images=10):\n",
    "    diff=0\n",
    "    onnx_correct=0\n",
    "    pytorch_correct=0\n",
    "    pytorch_model.eval()\n",
    "    for imagesvec, labelsvec in test_loader:\n",
    "      for i,images in enumerate(imagesvec):\n",
    "        label=labelsvec[i]\n",
    "        outputs_pytorch = model(images)[0].detach().numpy()\n",
    "        images_data = images.view(1, -1).numpy()  # Flatten the image\n",
    "        data = {next(iter(input_tensors)): images_data}\n",
    "        outputs_onnx=run_model(onnx_layers,onnx_initializers,data)\n",
    "        outputs_onnx=outputs_onnx['8'][0]\n",
    "        diff=diff+np.linalg.norm(outputs_onnx-outputs_pytorch)\n",
    "        label_onnx=np.argmax(outputs_onnx)\n",
    "        label_pytorch=np.argmax(outputs_pytorch)\n",
    "        if (label==label_onnx):\n",
    "            onnx_correct=onnx_correct+1\n",
    "        if (label==label_pytorch):\n",
    "            pytorch_correct=pytorch_correct+1\n",
    "    print('Average difference: ',diff/len(test_loader)/batch_size)\n",
    "    print('Ratio of correct labels for onnx: ',onnx_correct/(len(test_loader)*batch_size)*100)\n",
    "    print('Ratio of correct labels for Pytorch: ',pytorch_correct/(len(test_loader)*batch_size)*100)\n",
    "\n",
    "compare_models(layers,initializers,model,test_loader)\n",
    "compare_models(layers,initializers,model,train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134decd4-2b4b-4242-a6f3-a520daa4dfe9",
   "metadata": {},
   "source": [
    "## Understanding the weights\n",
    "Let us now investigate the first Gemm node and try to understand it. For the major part, this is just matrix multiplication of the input tensor A and the weights matrix B (note that this is stored transposed). \n",
    "\n",
    "Matrix multiplication can be thought of as taking the scalar product of each row of A with every column of B (again, note that these columns are stored as rows). Due to Cauchy-Schwartz, this scalar product is maximal when one row is generated from the other by multiplying with a positive factor. Remembering that the input is an image, the columns of B can be interpreted as prototype images or features.\n",
    "\n",
    "So, the scalar product can be interpreted as a very simple similarity measurement. When the scalar product is large, the similarity between image and feature is high. When it is very small (negative), the similarity is also large, but reversed (black and white interchanged).\n",
    "\n",
    "Using this interpretation, we can easily understand what is going on here. In the first Gemm, we have 128 images for which a similarity measurement towards the input is taken. The ReLU then cuts off the reverse similarity (ignoring the bias here). The second Gemm network then decides which features are characteristic for a number and assigns an importance weight to it.\n",
    "\n",
    "We now test this hypothesis first by displaying the columns of B as images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385c3a0-0a52-4ff7-9385-8eb6ea9fedc1",
   "metadata": {},
   "source": [
    "### Visualize the columns of first weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa21cc3-0fe7-4e86-a676-0afed4364f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights and visualize the columns\n",
    "def extract_weights(onnx_model, layer_name=None):\n",
    "    # Get graph from the model\n",
    "    graph = onnx_model.graph\n",
    "    initializers = {initializer.name: initializer for initializer in graph.initializer}\n",
    "\n",
    "    # Find the weight matrix for the specified layer (e.g., first GEMM layer)\n",
    "    if layer_name is None:\n",
    "        layer_name = list(initializers.keys())[0]  # Default to the first initializer if none specified\n",
    "    print(f\"Using {layer_name}\")\n",
    "    \n",
    "    weight_initializer = initializers[layer_name]\n",
    "    weights = np.frombuffer(weight_initializer.raw_data, dtype=np.float32).reshape(tuple(weight_initializer.dims))\n",
    "    return weights\n",
    "\n",
    "def visualize_weights(weights, input_size, num_classes, images_per_row=7, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the weights as prototype images.\n",
    "\n",
    "    :param weights: Weight matrix to visualize (classes x features)\n",
    "    :param input_size: Flattened input size (e.g., 784 for MNIST)\n",
    "    :param num_classes: Number of classes to visualize\n",
    "    :param images_per_row: Number of images to display per row\n",
    "    :param save_path: Path to save the visualization (optional)\n",
    "    \"\"\"\n",
    "    img_shape = (int(np.sqrt(input_size)), int(np.sqrt(input_size)))\n",
    "    assert np.prod(img_shape) == input_size, \"Input size does not match expected image dimensions.\"\n",
    "\n",
    "    # Normalize the weights for better visualization\n",
    "    normalized_weights = (weights - np.min(weights, axis=1, keepdims=True)) / \\\n",
    "                         (np.max(weights, axis=1, keepdims=True) - np.min(weights, axis=1, keepdims=True))\n",
    "\n",
    "    # Determine number of rows and columns\n",
    "    rows = (num_classes + images_per_row - 1) // images_per_row\n",
    "    cols = min(images_per_row, num_classes)\n",
    "\n",
    "    # Create the figure\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    axes = axes.flatten() if rows > 1 else [axes]\n",
    "\n",
    "    # Plot each class's weights\n",
    "    for i in range(num_classes):\n",
    "        ax = axes[i]\n",
    "        img = normalized_weights[i].reshape(img_shape)\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.set_title(f\"Row {i}\",fontsize=30)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    # Turn off unused subplots\n",
    "    for j in range(num_classes, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "weights = extract_weights(onnx_model,layer_name=\"fc1.weight\")\n",
    "\n",
    "# Define MNIST-specific parameters\n",
    "input_size = weights.shape[1]  # Flattened size of the input\n",
    "num_classes = weights.shape[0]  # Number of output classes\n",
    "if (input_size!=784):\n",
    "    input_size, num_classes=num_classes, input_size\n",
    "    weights=weights.T\n",
    "# Visualize weights as prototype images\n",
    "visualize_weights(weights, input_size, num_classes, images_per_row=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390b530-fa77-44b7-9f16-0ee0ade00e58",
   "metadata": {},
   "source": [
    "### Interpreting the images\n",
    "We see that mostly, our interpretation was correct. Many columns represent prototype images or features that are in some way contained in the images. We will come back to this later.\n",
    "\n",
    "However, many images just contain noise. They obviously provide no usable information at all. In fact, we should remove these from the model, since they obviously only give fake information.\n",
    "\n",
    "This removal process is called pruning, we will do this manually later. However, pruning is integrated into all major software packages, particularly pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ca80e-2288-42fe-b0df-9a4913474a00",
   "metadata": {},
   "source": [
    "### Checking the most important features\n",
    "We now take a random image from the set and try to understand how it was classified. To do this, we run the image through the simulated network. Then, we take the output of the ReLU node and examine the application of weights B.\n",
    "We start by computing the output value for a random image. Note that since the image is always different, you have to interpret yourself what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7cedb-75f7-4b87-9fa7-72f82874ebc5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Importance check\n",
    "input_tensor=next(iter(input_tensors))\n",
    "B=extract_weights(onnx_model,'fc2.weight')\n",
    "bias=extract_weights(onnx_model,'fc2.bias')\n",
    "B=B.transpose()\n",
    "images_vec,labels_vec=next(iter(test_loader))\n",
    "image=images_vec[0]\n",
    "label=labels_vec[0].numpy()\n",
    "input_data = image.view(1, -1).numpy()  # Flatten the image\n",
    "data={input_tensor:input_data}\n",
    "result=run_model(layers, initializers, data)\n",
    "arr=result['8'][0]\n",
    "print(f'Correct index is {label}\\n')\n",
    "# 1. Get indices that would sort arr by descending magnitude\n",
    "sorted_indices = np.argsort((arr))[::-1]\n",
    "\n",
    "# 2. Print the result as index:value pairs\n",
    "for idx in sorted_indices:\n",
    "    print(f\"{idx}:{arr[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e853cd54-0cb0-47f9-ad89-43249be0625b",
   "metadata": {},
   "source": [
    "We do the last step again manually using B and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10ee76-11ba-4328-b716-50b296f6ee30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute results in numpy\n",
    "relu_output=result['/relu/Relu_output_0'][0]\n",
    "manually=relu_output.dot(B)+bias\n",
    "arr=manually\n",
    "# 1. Get indices that would sort arr by descending magnitude\n",
    "sorted_indices = np.argsort((arr))[::-1]\n",
    "# 2. Print the result as index:value pairs\n",
    "for idx in sorted_indices:\n",
    "    print(f\"{idx}:{arr[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e1b77-4866-4f8f-a9db-46dab24acfa8",
   "metadata": {},
   "source": [
    "Ok, so our interpretation is correct. Now we go deeper. Since the bias in this case is more or less negligible, we do exactly that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ce3eb-cd84-44d4-993b-e0b51c7b3291",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute top contributing features for each output\n",
    "def largest_contributors_sorted_by_output(a, B, top_n=3):\n",
    "    \"\"\"\n",
    "    a: shape (128,)\n",
    "    B: shape (128, 10)\n",
    "    top_n: number of top contributors for each output dimension.\n",
    "\n",
    "    Returns a list of dictionaries, one per output dimension, in descending order\n",
    "    of the output value. Each dictionary has:\n",
    "      - \"output_index\": int, the index of this output dimension\n",
    "      - \"output_value\": float, the computed y[output_index]\n",
    "      - \"top_contributors\": list of (index, contribution) sorted by descending contribution\n",
    "    \"\"\"\n",
    "    # 1. Compute the output vector y = a @ B -> shape: (10,)\n",
    "    y = a @ B\n",
    "    \n",
    "    # 2. Sort outputs by descending value\n",
    "    sorted_output_indices = np.argsort(y)[::-1]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # 3. For each output dimension in descending order\n",
    "    for out_idx in sorted_output_indices:\n",
    "        # partial_products[k] = a[k] * B[k, out_idx]\n",
    "        partial_products = a * B[:, out_idx]\n",
    "        \n",
    "        # Sort partial products in descending order\n",
    "        sorted_indices = np.argsort(partial_products)[::-1]\n",
    "        top_indices = sorted_indices[:top_n]\n",
    "        \n",
    "        # Build a list of (index, contribution)\n",
    "        top_list = [(int(idx), float(partial_products[idx])) for idx in top_indices]\n",
    "        \n",
    "        # Collect result in a dictionary\n",
    "        results.append({\n",
    "            \"output_index\": int(out_idx),\n",
    "            \"output_value\": float(y[out_idx]),\n",
    "            \"top_contributors\": top_list\n",
    "        })\n",
    "\n",
    "    for elem in results:\n",
    "        out_idx = elem[\"output_index\"]\n",
    "        out_val = elem[\"output_value\"]\n",
    "        top_list = elem[\"top_contributors\"]\n",
    "        \n",
    "        # One-line output: output index, value, top contributors\n",
    "        contributors_str = \", \".join(f\"{idx}:{val:.4f}\" for idx, val in top_list)\n",
    "        print(f\"y[{out_idx}] = {out_val:.4f}, top {len(top_list)} => {contributors_str}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "contribs = largest_contributors_sorted_by_output(relu_output, B, top_n=5)\n",
    "\n",
    "    # Example of how you might print the condensed result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024101d8-f1ee-4347-97af-05446ccce3a0",
   "metadata": {},
   "source": [
    "Now take the image of the top contributor for your image. In my test, this is really close to a 5 which was the correct answer. Do this several times, and you get a feeling for it. To make this easier, we display the original image, and the top two contributors.\n",
    "\n",
    "Note that we ignored the bias, so the results are slightly off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91972b7a-45ab-4ed8-8b29-76bd1e922ce1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Show top contributors visually\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_four_images_side_by_side(vec1, vec2, vec3, vec4,\n",
    "                                  title1=\"Image 1\", \n",
    "                                  title2=\"Image 2\", \n",
    "                                  title3=\"Image 3\", \n",
    "                                  title4=\"Image 4\"):\n",
    "    \"\"\"\n",
    "    Each vecX is assumed to be a 1D numpy array of length 784 (28x28).\n",
    "    Displays them side by side with given titles.\n",
    "    \"\"\"\n",
    "    # Reshape each vector into 28x28\n",
    "    img1 = vec1.reshape(28, 28)\n",
    "    img2 = vec2.reshape(28, 28)\n",
    "    img3 = vec3.reshape(28, 28)\n",
    "    img4 = vec4.reshape(28, 28)\n",
    "    \n",
    "    # Create subplots: 1 row, 4 columns\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "    \n",
    "    # Display each image in grayscale\n",
    "    axes[0].imshow(img1, cmap='gray')\n",
    "    axes[0].set_title(title1,fontsize=30)\n",
    "    axes[0].axis(\"off\")  # Hide axis ticks\n",
    "    \n",
    "    axes[1].imshow(img2, cmap='gray')\n",
    "    axes[1].set_title(title2,fontsize=30)\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    axes[2].imshow(img3, cmap='gray')\n",
    "    axes[2].set_title(title3,fontsize=30)\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    axes[3].imshow(img4, cmap='gray')\n",
    "    axes[3].set_title(title4,fontsize=30)\n",
    "    axes[3].axis(\"off\")\n",
    "    \n",
    "    # Adjust layout for clarity\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with dummy data:\n",
    "im=image.numpy()\n",
    "top1=contribs[0]['top_contributors'][0][0]\n",
    "top2=contribs[1]['top_contributors'][1][0]\n",
    "top3=contribs[2]['top_contributors'][2][0]\n",
    "show_four_images_side_by_side(im,weights[top1,:],weights[top2,:],weights[top3,:],\n",
    "                             'Image','Top 1','Top 2','Top 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb86a72-adbc-439f-b368-6fc4c67cad30",
   "metadata": {},
   "source": [
    "Note that this interpretation is very far from perfect. In particular, one needs to adress also the other elements. It is not only that one class is chosen as superior, but it might also be that just all other classes are impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed1368-28c4-4b5a-a4d3-e1285e85d704",
   "metadata": {},
   "source": [
    "## Pruning results\n",
    "We already noticed that some of the internal notes seem to contribute only noise to the classification. In a pruning step, nodes that do not contribute or are bad contributors are removed from the network.\n",
    "In our simple network, the approach is simple. We analyze the columns of the first weight matrix. If these are random and not images, we delete them from the network.\n",
    "Alternatively, one could try to estimate what the contribution of each node to the result is, possibly by running a large test data set, and then remove the N most irrelevant nodes.\n",
    "We use both approaches. In the first one, we measure the \"imageness\" of the rows and remove the N most unlikely images.\n",
    "In the second, we try to estimate the relevance of a node by looking at the weight matrix. The results are mostly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af38bda-d31b-44c8-9165-3e28e9fb9e31",
   "metadata": {},
   "source": [
    "### Pruning approach 1: Measure imageness\n",
    "In this approach, we estimate the covariance of adjacent pixels in the image (very crude approach). Then, we remove the nodes that correspond to non-images.\n",
    "\n",
    "Note that this can be interpreted as taking the scalar product of the original image and one that was shifted one pixel up/down (see the feature discussion above).\n",
    "\n",
    "We work directly on the ONNX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2e8da-fae2-4d01-9c47-3f3a993fdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune by imageness\n",
    "def measure_imageness(row_vector, img_height=28, img_width=28):\n",
    "    \"\"\"\n",
    "    Given a 1D row_vector representing a flattened 2D image (28x28),\n",
    "    compute a simple ‚Äúimage-likeness‚Äù score.\n",
    "    For demonstration, we sum the products of horizontally and vertically\n",
    "    adjacent pixels. Larger values => more structured, less random/noisy.\n",
    "    \"\"\"\n",
    "    # Reshape the row into a 2D image\n",
    "    img = row_vector.reshape(img_height, img_width)\n",
    "\n",
    "    score = 0.0\n",
    "    # Horizontal neighbors\n",
    "    for i in range(img_height):\n",
    "        for j in range(img_width - 1):\n",
    "            score += img[i, j] * img[i, j+1]\n",
    "\n",
    "    # Vertical neighbors\n",
    "    for i in range(img_height - 1):\n",
    "        for j in range(img_width):\n",
    "            score += img[i, j] * img[i+1, j]\n",
    "\n",
    "    return score\n",
    "\n",
    "def prune_noisy_neurons(\n",
    "    model_path: str,\n",
    "    output_path: str,\n",
    "    num_to_remove: int = 45,\n",
    "    img_height: int = 28,\n",
    "    img_width: int = 28,\n",
    "    fc1_weight_name: str = \"fc1.weight\",\n",
    "    fc1_bias_name: str = \"fc1.bias\",\n",
    "    fc2_weight_name: str = \"fc2.weight\",\n",
    "    fc2_bias_name: str = \"fc2.bias\"\n",
    ") -> onnx.ModelProto:\n",
    "    \"\"\"\n",
    "    Load an ONNX model and prune rows (neurons) of fc1.weight that appear\n",
    "    to be \"noisy\" when interpreted as a 28x28 image. By default, removes 45 neurons.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load the ONNX model\n",
    "    model = onnx.load(model_path)\n",
    "    graph = model.graph\n",
    "\n",
    "    # Helper function: retrieve initializer by name\n",
    "    def get_param(name):\n",
    "        for init in graph.initializer:\n",
    "            if init.name == name:\n",
    "                return init\n",
    "        raise ValueError(f\"Initializer '{name}' not found in the model.\")\n",
    "\n",
    "    # 2. Extract initializers as NumPy arrays\n",
    "    fc1_weight_init = get_param(fc1_weight_name)\n",
    "    fc1_bias_init   = get_param(fc1_bias_name)\n",
    "    fc2_weight_init = get_param(fc2_weight_name)\n",
    "    fc2_bias_init   = get_param(fc2_bias_name)\n",
    "\n",
    "    fc1_weight = numpy_helper.to_array(fc1_weight_init)  # shape: (hidden_size, 784)\n",
    "    fc1_bias   = numpy_helper.to_array(fc1_bias_init)    # shape: (hidden_size,)\n",
    "    fc2_weight = numpy_helper.to_array(fc2_weight_init)  # shape: (output_size, hidden_size)\n",
    "    fc2_bias   = numpy_helper.to_array(fc2_bias_init)    # shape: (output_size,)\n",
    "\n",
    "    # Check that each row has length = 28*28\n",
    "    hidden_size, in_features = fc1_weight.shape\n",
    "    expected_size = img_height * img_width\n",
    "    if in_features != expected_size:\n",
    "        raise ValueError(\n",
    "            f\"Expected fc1_weight to have {expected_size} in_features, \"\n",
    "            f\"but got shape {fc1_weight.shape}.\"\n",
    "        )\n",
    "\n",
    "    # 3. Compute an ‚Äúimage-likeness‚Äù score for each row (neuron)\n",
    "    scores = []\n",
    "    for row_idx in range(hidden_size):\n",
    "        row_vector = fc1_weight[row_idx, :]  # shape [784]\n",
    "        score = measure_imageness(row_vector, img_height, img_width)\n",
    "        scores.append(score)\n",
    "\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # 4. Identify the neurons with the lowest scores => \"noisy\"\n",
    "    if num_to_remove >= hidden_size:\n",
    "        raise ValueError(f\"Cannot remove {num_to_remove} neurons from only {hidden_size}.\")\n",
    "    sorted_indices = np.argsort(scores)  # ascending order\n",
    "    remove_indices = sorted_indices[:num_to_remove]\n",
    "    keep_indices   = sorted_indices[num_to_remove:]\n",
    "\n",
    "    # 5. Prune from fc1_weight, fc1_bias, fc2_weight\n",
    "    #    - Removing neurons => remove those rows in fc1_weight\n",
    "    #    - fc1_bias => remove the same entries\n",
    "    #    - fc2_weight => remove corresponding columns\n",
    "    pruned_fc1_weight = fc1_weight[keep_indices, :]\n",
    "    pruned_fc1_bias   = fc1_bias[keep_indices]\n",
    "    # fc2_weight shape: (output_size, hidden_size)\n",
    "    # removing neurons => remove same columns\n",
    "    pruned_fc2_weight = fc2_weight[:, keep_indices]\n",
    "\n",
    "    # fc2_bias remains unchanged (size = output_size)\n",
    "\n",
    "    # 6. Update the initializers\n",
    "    def set_param(param_init, new_array):\n",
    "        new_tensor = numpy_helper.from_array(new_array, param_init.name)\n",
    "        param_init.ClearField(\"float_data\")\n",
    "        param_init.ClearField(\"int32_data\")\n",
    "        param_init.ClearField(\"int64_data\")\n",
    "        param_init.raw_data = new_tensor.raw_data\n",
    "        param_init.dims[:] = new_array.shape\n",
    "\n",
    "    set_param(fc1_weight_init, pruned_fc1_weight)\n",
    "    set_param(fc1_bias_init,   pruned_fc1_bias)\n",
    "    set_param(fc2_weight_init, pruned_fc2_weight)\n",
    "    # fc2_bias unchanged\n",
    "\n",
    "    # 7. Save the pruned model\n",
    "    onnx.save(model, output_path)\n",
    "    print(f\"Pruned {num_to_remove} ‚Äònoisy‚Äô neurons. Saved to '{output_path}'.\")\n",
    "    return model\n",
    "\n",
    "prune_model_1=prune_noisy_neurons(\n",
    "        model_path=\"model.onnx\",\n",
    "        output_path=\"model_pruned_imageness.onnx\",\n",
    "        num_to_remove=48  # default\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fa1bc-d7c3-4477-a2be-6468c2f138bc",
   "metadata": {},
   "source": [
    "We print out the model and compare.\n",
    "\n",
    "Note that the number of neurons in the hidden layewr has been reduced to 80, but the success rates do not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54edb0-d584-431f-954a-4b9e6d4726a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Display parameters of pruned model\n",
    "prune_layers, prune_input_tensors, prune_output_tensors, prune_initializers = extract_model_info(prune_model_1)\n",
    "print_onnx_info(prune_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24756e1e-1ae2-469a-abc5-75064f9eca89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# compare fully connected and imageness\n",
    "print('fully connected model')\n",
    "compare_models(layers,initializers,model,test_loader)\n",
    "print('imageness pruned model')\n",
    "compare_models(prune_layers,prune_initializers,model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934b779-a2f4-4148-8405-b89d18c168da",
   "metadata": {},
   "source": [
    "### Pruning approach 2: estimate relevance\n",
    "In this approach, we simply add up the contributing weights for each neuron in the hidden layer and remove those with the smalles weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc365c3-9e8e-430c-8ec1-33ad7c6f95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune by weight\n",
    "def prune_hidden_neurons(\n",
    "    model_path: str,\n",
    "    output_path: str,\n",
    "    n_to_prune: int\n",
    ") -> onnx.ModelProto:\n",
    "    \"\"\"\n",
    "    Load an ONNX model, prune N neurons from the hidden layer, \n",
    "    and save the pruned model to output_path.\n",
    "    Returns the pruned model object.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load ONNX model\n",
    "    model = onnx.load(model_path)\n",
    "    graph = model.graph\n",
    "\n",
    "    # 2. Identify weight/bias initializers for fc1 and fc2.\n",
    "    #    Change these names if your ONNX export uses different ones.\n",
    "    fc1_weight_name = \"fc1.weight\"\n",
    "    fc1_bias_name   = \"fc1.bias\"\n",
    "    fc2_weight_name = \"fc2.weight\"\n",
    "    fc2_bias_name   = \"fc2.bias\"\n",
    "\n",
    "    # Helper function to grab an initializer's numpy array\n",
    "    def get_param(name):\n",
    "        for init in graph.initializer:\n",
    "            if init.name == name:\n",
    "                return init\n",
    "        raise ValueError(f\"Initializer {name} not found in the model.\")\n",
    "\n",
    "    # 3. Convert the relevant initializers to numpy arrays\n",
    "    fc1_weight_init = get_param(fc1_weight_name)\n",
    "    fc1_bias_init   = get_param(fc1_bias_name)\n",
    "    fc2_weight_init = get_param(fc2_weight_name)\n",
    "    fc2_bias_init   = get_param(fc2_bias_name)\n",
    "\n",
    "    fc1_weight = numpy_helper.to_array(fc1_weight_init)  # shape [hidden_size, input_size]\n",
    "    fc1_bias   = numpy_helper.to_array(fc1_bias_init)    # shape [hidden_size]\n",
    "    fc2_weight = numpy_helper.to_array(fc2_weight_init)  # shape [output_size, hidden_size]\n",
    "    fc2_bias   = numpy_helper.to_array(fc2_bias_init)    # shape [output_size]\n",
    "\n",
    "    # Sanity check\n",
    "    hidden_size = fc1_bias.shape[0]\n",
    "    if n_to_prune >= hidden_size:\n",
    "        raise ValueError(f\"Cannot prune {n_to_prune} neurons from hidden layer of size {hidden_size}.\")\n",
    "\n",
    "    # 4. Compute saliency measure for each hidden neuron\n",
    "    #    Here, \"saliency\" = sum(|fc1_weight[i, :]|) + sum(|fc2_weight[:, i]|)\n",
    "    saliencies = (\n",
    "        # np.abs(fc1_weight).sum(axis=1)  # row i in fc1 (for hidden neuron i)\n",
    "        np.abs(fc2_weight).sum(axis=0)  # column i in fc2\n",
    "    )\n",
    "\n",
    "    # 5. Identify the neurons with the smallest saliency\n",
    "    prune_indices = np.argsort(saliencies)[:n_to_prune]\n",
    "    print(prune_indices)\n",
    "    print(saliencies[prune_indices])\n",
    "    keep_indices = [i for i in range(hidden_size) if i not in prune_indices]\n",
    "\n",
    "    # 6. Construct new pruned arrays\n",
    "    pruned_fc1_weight = fc1_weight[keep_indices, :]\n",
    "    pruned_fc1_bias   = fc1_bias[keep_indices]\n",
    "    pruned_fc2_weight = fc2_weight[:, keep_indices]\n",
    "    # fc2_bias is unchanged because it's [output_size] (no hidden-dim index)\n",
    "\n",
    "    # 7. Update the initializers in the graph\n",
    "    def set_param(param_init, new_array):\n",
    "        new_tensor = numpy_helper.from_array(new_array, param_init.name)\n",
    "        # Clear out the existing data fields\n",
    "        param_init.ClearField(\"float_data\")\n",
    "        param_init.ClearField(\"int32_data\")\n",
    "        param_init.ClearField(\"int64_data\")\n",
    "        # Assign the new raw_data and dimensions\n",
    "        param_init.raw_data = new_tensor.raw_data\n",
    "        param_init.dims[:] = new_array.shape\n",
    "\n",
    "    set_param(fc1_weight_init, pruned_fc1_weight)\n",
    "    set_param(fc1_bias_init,   pruned_fc1_bias)\n",
    "    set_param(fc2_weight_init, pruned_fc2_weight)\n",
    "    # fc2_bias remains as is\n",
    "\n",
    "    # 8. (Optional) Update shape info if your model has static shapes.\n",
    "    #    Here, we skip it, letting shape inference happen at runtime.\n",
    "\n",
    "    # 9. Save the pruned model\n",
    "    onnx.save(model, output_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "prune_model_2 = prune_hidden_neurons(\n",
    "        model_path=\"model.onnx\",\n",
    "        output_path=\"model_pruned_magnitude.onnx\",\n",
    "        n_to_prune=48  # number of neurons to remove from hidden layer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872febaa-c33d-43f3-86ca-5771ab4620a3",
   "metadata": {},
   "source": [
    "Analyze as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172d5a2-407b-482c-9bb5-16a4ac054402",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print model info\n",
    "prune_layers2, prune_input_tensors2, prune_output_tensors2, prune_initializers2 = extract_model_info(prune_model_2)\n",
    "print_onnx_info(prune_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04dc98-688f-4ca3-a488-87d51319ca00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print('fully connected model')\n",
    "compare_models(layers,initializers,model,test_loader)\n",
    "print('pruned by imageness')\n",
    "compare_models(prune_layers,prune_initializers,model, test_loader)\n",
    "print('pruned by weight')\n",
    "compare_models(prune_layers2,prune_initializers2,model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b816146-8024-488f-83f9-3e6823c7f723",
   "metadata": {},
   "source": [
    "Finally, we check that we removed the correct nodes from the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59de5db-dc2e-41d7-866b-bf8abc88a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show imageness pruned weights\n",
    "weights_prune = extract_weights(prune_model_1)\n",
    "# Define MNIST-specific parameters\n",
    "input_size = weights_prune.shape[1]  # Flattened size of the input\n",
    "num_classes = weights_prune.shape[0]  # Number of output classes\n",
    "\n",
    "# Visualize weights as prototype images\n",
    "visualize_weights(weights_prune, input_size, num_classes, images_per_row=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001f280-0d1d-4fad-83ad-59c984409b41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Show weights pruned images\n",
    "weights_prune = extract_weights(prune_model_2)\n",
    "# Define MNIST-specific parameters\n",
    "input_size = weights_prune.shape[1]  # Flattened size of the input\n",
    "num_classes = weights_prune.shape[0]  # Number of output classes\n",
    "\n",
    "# Visualize weights as prototype images\n",
    "visualize_weights(weights_prune, input_size, num_classes, images_per_row=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c45315-bc01-4457-bfe0-e4c0736d12f5",
   "metadata": {},
   "source": [
    "We see that the imageness approach worked, while the magnitude approach left two noise images in the network (and removed two relevant ones). However, the success range hardly changed. Again, since everything in here is completely random, your mileage may vary when running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af3f16-7f1a-4b06-8042-eea65cd4fc31",
   "metadata": {},
   "source": [
    "## Gradient computation\n",
    "We now investigate how the weights are learned. Again, we switch to our simple model from the beginning. First thing, we need to define gradient computation for the network.\n",
    "\n",
    "### Simplify ONNX model\n",
    "We start by simplifying the ONNX model by explicitly transposing the internal B matrices of the GEMM nodes, so now transB and transA are always false. We can leave them out in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81635254-594b-4ac5-9a2d-05345a617ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from onnx import numpy_helper\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load('model.onnx')\n",
    "\n",
    "# Function to normalize GEMM nodes by transposing weights if transB is True\n",
    "def normalize_gemm_nodes(model):\n",
    "    \"\"\"\n",
    "    Normalize the GEMM nodes in the ONNX model by transposing weight matrices\n",
    "    where transB is set to True, and setting transB to False.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The ONNX model to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    - normalized_model: The normalized ONNX model.\n",
    "    \"\"\"\n",
    "    graph = model.graph\n",
    "\n",
    "    # Helper function to find an initializer by name\n",
    "    def find_initializer(name):\n",
    "        for initializer in graph.initializer:\n",
    "            if initializer.name == name:\n",
    "                return initializer\n",
    "        return None\n",
    "\n",
    "    # Iterate over all nodes in the graph\n",
    "    for node in graph.node:\n",
    "        if node.op_type == 'Gemm':\n",
    "            # Check for the transB attribute\n",
    "            transB_attr = next((attr for attr in node.attribute if attr.name == 'transB'), None)\n",
    "            transB = transB_attr.i if transB_attr else 0\n",
    "\n",
    "            if transB == 1:\n",
    "                # Find the weight initializer (second input of Gemm)\n",
    "                weight_name = node.input[1]\n",
    "                weight_initializer = find_initializer(weight_name)\n",
    "\n",
    "                if weight_initializer:\n",
    "                    # Convert the initializer to a numpy array\n",
    "                    weight_array = numpy_helper.to_array(weight_initializer)\n",
    "\n",
    "                    # Transpose the weight array\n",
    "                    transposed_weight_array = weight_array.T\n",
    "\n",
    "                    # Create a new initializer with the transposed weights\n",
    "                    new_weight_initializer = numpy_helper.from_array(transposed_weight_array, name=weight_name)\n",
    "\n",
    "                    # Replace the old initializer with the new one\n",
    "                    graph.initializer.remove(weight_initializer)\n",
    "                    graph.initializer.append(new_weight_initializer)\n",
    "\n",
    "                    # Set transB to 0 (False)\n",
    "                    transB_attr.i = 0\n",
    "\n",
    "    return model\n",
    "\n",
    "# Normalize the GEMM nodes\n",
    "onnx_model = normalize_gemm_nodes(onnx_model)\n",
    "\n",
    "# Save the normalized model if needed\n",
    "onnx.save(onnx_model, 'normalized_model.onnx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20570c79-c785-474e-b899-0c1941e854e1",
   "metadata": {},
   "source": [
    "### Forward and backward pass\n",
    "The forward pass is the same as above, only this time we leave out transB etc. Also, we allow for the input to be a tensor (batch of images).\n",
    "\n",
    "In the backward pass, we compute the gradient of the loss function according to the chain rule. We obviously have\n",
    "\n",
    "$$\n",
    "\\frac{d}{db} f_1(a,f_2(b,x))=(f_1)_y(a,f_2(b,x))\\cdot (f_2)_x(b,x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c9d27-e320-4e5b-a9a7-df3ffdc3eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gemm_forward(A, B, C=None, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Perform the forward pass of the GEMM operation for a batch of inputs.\n",
    "\n",
    "    Parameters:\n",
    "    - A: Input tensor of shape (N, input_features).\n",
    "    - B: Weight matrix of shape (input_features, output_features).\n",
    "    - C: Bias vector of shape (output_features,) or None.\n",
    "    - alpha: Scalar multiplier for the product of A and B.\n",
    "    - beta: Scalar multiplier for the bias C.\n",
    "\n",
    "    Returns:\n",
    "    - Y: Output tensor of shape (N, output_features).\n",
    "    \"\"\"\n",
    "    Y = alpha * np.dot(A, B)\n",
    "    if C is not None:\n",
    "        Y += beta * C\n",
    "    return Y\n",
    "\n",
    "def gemm_backward(dY, A, B, C, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the GEMM operation with respect to A, B, and C for a batch of inputs.\n",
    "\n",
    "    Parameters:\n",
    "    - dY: Gradient of the loss with respect to the output Y, shape (N, output_features).\n",
    "    - A: Input tensor of shape (N, input_features).\n",
    "    - B: Weight matrix of shape (input_features, output_features).\n",
    "    - alpha: Scalar multiplier for the product of A and B.\n",
    "    - beta: Scalar multiplier for the bias C.\n",
    "\n",
    "    Returns:\n",
    "    - dA: Gradient with respect to A, shape (N, input_features).\n",
    "    - dB: Gradient with respect to B, shape (input_features, output_features).\n",
    "    - dC: Gradient with respect to C, shape (output_features,).\n",
    "    \"\"\"\n",
    "    dA = alpha * np.dot(dY, B.T)\n",
    "    dB = alpha * np.dot(A.T, dY)\n",
    "    dC = beta * np.sum(dY, axis=0) if beta != 0 else None\n",
    "    return dA, dB, dC\n",
    "\n",
    "def relu_forward(X):\n",
    "    \"\"\"\n",
    "    Perform the forward pass of the ReLU activation function for a batch of inputs.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input tensor of shape (N, features).\n",
    "\n",
    "    Returns:\n",
    "    - Y: Output tensor after applying ReLU, same shape as X.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "def relu_backward(dY, X):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the ReLU activation function with respect to its input for a batch of inputs.\n",
    "\n",
    "    Parameters:\n",
    "    - dY: Gradient of the loss with respect to the output of the ReLU layer, shape (N, features).\n",
    "    - X: Input to the ReLU layer, shape (N, features).\n",
    "\n",
    "    Returns:\n",
    "    - dX: Gradient of the loss with respect to the input of the ReLU layer, same shape as X.\n",
    "    \"\"\"\n",
    "    return dY * (X > 0)\n",
    "\n",
    "def flatten_forward(X, axis=1):\n",
    "    \"\"\"\n",
    "    Perform the forward pass of the Flatten operation for a batch of inputs.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input tensor of shape (N, ...).\n",
    "    - axis: The axis to flatten from.\n",
    "\n",
    "    Returns:\n",
    "    - Y: Flattened output tensor of shape (N, new_features).\n",
    "    - original_shape: The original shape of the input tensor.\n",
    "    \"\"\"\n",
    "    original_shape = X.shape\n",
    "    new_shape = (original_shape[0], -1)\n",
    "    Y = X.reshape(new_shape)\n",
    "    return Y, original_shape\n",
    "\n",
    "def flatten_backward(dY, original_shape):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the Flatten operation with respect to its input for a batch of inputs.\n",
    "\n",
    "    Parameters:\n",
    "    - dY: Gradient of the loss with respect to the flattened output, shape (N, new_features).\n",
    "    - original_shape: The original shape of the input tensor before flattening.\n",
    "\n",
    "    Returns:\n",
    "    - dX: Gradient of the loss with respect to the input of the Flatten layer, shape same as original_shape.\n",
    "    \"\"\"\n",
    "    return dY.reshape(original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99823f-c8e1-4d63-8dea-4c3b294cc3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward and backward pass storage\n",
    "activations = {}\n",
    "original_shapes = {}\n",
    "gradients = defaultdict(lambda: None)\n",
    "\n",
    "def set_parameter(onnx_model, param_name, new_value):\n",
    "    \"\"\"\n",
    "    Overwrites the initializer in onnx_model that has name == param_name\n",
    "    with the NumPy array new_value.\n",
    "    \n",
    "    onnx_model : an ONNX model (e.g. returned by onnx.load(...))\n",
    "    param_name : string name of the parameter to update\n",
    "    new_value  : a NumPy array with the same shape and dtype as the initializer\n",
    "    \"\"\"\n",
    "    found = False\n",
    "    for initializer in onnx_model.graph.initializer:\n",
    "        if initializer.name == param_name:\n",
    "            # Convert new_value to a TensorProto\n",
    "            new_init = numpy_helper.from_array(new_value, param_name)\n",
    "            initializer.CopyFrom(new_init)\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        raise ValueError(f\"Parameter '{param_name}' not found in model initializers.\")\n",
    "\n",
    "# Helper function to get initializers (weights and biases)\n",
    "def get_initializer(name):\n",
    "    for initializer in onnx_model.graph.initializer:\n",
    "        if initializer.name == name:\n",
    "            return onnx.numpy_helper.to_array(initializer)\n",
    "    return None\n",
    "\n",
    "# Helper function to get node attributes\n",
    "def get_attribute(node, attr_name, default=None):\n",
    "    for attr in node.attribute:\n",
    "        if attr.name == attr_name:\n",
    "            return onnx.helper.get_attribute_value(attr)\n",
    "    return default\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(input_data):\n",
    "    global activations\n",
    "    input_name=onnx_model.graph.input[0].name\n",
    "    activations[input_name] = input_data\n",
    "    for node in onnx_model.graph.node:\n",
    "        if node.op_type == 'Flatten':\n",
    "            X = activations[node.input[0]]\n",
    "            axis = get_attribute(node, 'axis', 1)\n",
    "            Y, original_shape = flatten_forward(X, axis)\n",
    "            activations[node.output[0]] = Y\n",
    "            original_shapes[node.output[0]] = original_shape\n",
    "        elif node.op_type == 'Gemm':\n",
    "            A = activations[node.input[0]]\n",
    "            B = get_initializer(node.input[1])\n",
    "            C = get_initializer(node.input[2]) if len(node.input) > 2 else None\n",
    "            alpha = get_attribute(node, 'alpha', 1.0)\n",
    "            beta = get_attribute(node, 'beta', 1.0)\n",
    "            Y = gemm_forward(A, B, C, alpha, beta)\n",
    "            activations[node.output[0]] = Y\n",
    "        elif node.op_type == 'Relu':\n",
    "            X = activations[node.input[0]]\n",
    "            Y = relu_forward(X)\n",
    "            activations[node.output[0]] = Y\n",
    "    return activations\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(dLoss_dOutput):\n",
    "    output_name=onnx_model.graph.output[0].name\n",
    "    gradients[output_name]=dLoss_dOutput\n",
    "    for node in reversed(onnx_model.graph.node):\n",
    "        if node.op_type == 'Relu':\n",
    "            X = activations[node.input[0]]\n",
    "            dY = gradients[node.output[0]]\n",
    "            dX = relu_backward(dY, X)\n",
    "            gradients[node.input[0]] = dX\n",
    "        elif node.op_type == 'Gemm':\n",
    "            A = activations[node.input[0]]\n",
    "            B = get_initializer(node.input[1])\n",
    "            C = get_initializer(node.input[2])\n",
    "            dY = gradients[node.output[0]]\n",
    "            dA, dB, dC = gemm_backward(dY, A, B, C)\n",
    "            gradients[node.input[0]] = dA\n",
    "            gradients[node.input[1]] = dB\n",
    "            gradients[node.input[2]] = dC\n",
    "        elif node.op_type == 'Flatten':\n",
    "            dY = gradients[node.output[0]]\n",
    "            original_shape = original_shapes[node.output[0]]\n",
    "            dX = flatten_backward(dY, original_shape)\n",
    "            gradients[node.input[0]] = dX\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e7af7-c521-4923-8085-600b1dd7c521",
   "metadata": {},
   "source": [
    "### Check gradients with finite differences\n",
    "We check that gradient computation is correct using finite differences and an l2 loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54867ef2-0aae-4cc4-bdd1-94b0f5e31512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following, outputs has shape (N,10) *and* labels has shape (N,10), typically \"one-hot\"\n",
    "def l2loss(outputs,labels):\n",
    "    diff=outputs-labels\n",
    "    loss = np.sum(diff*diff)/2\n",
    "    return loss/outputs.shape[0]\n",
    "def l2loss_grad(outputs,labels):\n",
    "    diff=outputs-labels\n",
    "    return diff/outputs.shape[0]\n",
    "def softmax_cross_entropy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Computes the average softmax cross-entropy loss for one-hot labels.\n",
    "    \n",
    "    outputs: np.ndarray, shape (batch_size, num_classes)\n",
    "             The raw logits (unnormalized scores).\n",
    "    labels:  np.ndarray, same shape as outputs\n",
    "             One-hot labels.\n",
    "    Returns:\n",
    "        float: The average cross-entropy loss over the batch.\n",
    "    \"\"\"\n",
    "    # Shift logits to improve numerical stability\n",
    "    shifted_outputs = outputs - np.max(outputs, axis=1, keepdims=True)\n",
    "    exp_outputs = np.exp(shifted_outputs)\n",
    "    probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "    \n",
    "    # Cross-entropy = -sum( labels * log(probs) )\n",
    "    # Then we average over batch_size\n",
    "    log_probs = np.log(probs + 1e-15)  # small constant to avoid log(0)\n",
    "    loss = -np.sum(labels * log_probs)\n",
    "    return loss / outputs.shape[0]\n",
    "def softmax_cross_entropy_grad(outputs, labels):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the softmax cross-entropy loss w.r.t. 'outputs'.\n",
    "    \n",
    "    outputs: np.ndarray, shape (batch_size, num_classes)\n",
    "    labels:  np.ndarray, same shape as outputs (one-hot labels).\n",
    "    Returns:\n",
    "        np.ndarray: Gradient of shape (batch_size, num_classes).\n",
    "    \"\"\"\n",
    "    # Same softmax calculation\n",
    "    shifted_outputs = outputs - np.max(outputs, axis=1, keepdims=True)\n",
    "    exp_outputs = np.exp(shifted_outputs)\n",
    "    probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "    \n",
    "    # Gradient is (softmax - labels) / batch_size\n",
    "    grad = (probs - labels) / outputs.shape[0]\n",
    "    return grad\n",
    "def current_loss(outputs,labels):\n",
    "    return softmax_cross_entropy(outputs,labels)\n",
    "def current_loss_grad(outputs,labels):\n",
    "    return softmax_cross_entropy_grad(outputs,labels)\n",
    "def generate_random_one_hot_data(N, num_classes=10):\n",
    "    \"\"\"\n",
    "    Generates a random one-hot array of shape (N, num_classes).\n",
    "    Each row has exactly one '1' corresponding to a random class.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    N : int\n",
    "        Number of samples in the batch.\n",
    "    num_classes : int\n",
    "        Number of classes (default: 10).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    one_hot_labels : np.ndarray of shape (N, num_classes)\n",
    "        The generated one-hot labels.\n",
    "    \"\"\"\n",
    "    # Generate random integer labels in [0, num_classes)\n",
    "    random_labels = np.random.randint(low=0, high=num_classes, size=N)\n",
    "\n",
    "    # Initialize the one-hot array\n",
    "    one_hot_labels = np.zeros((N, num_classes), dtype=np.float32)\n",
    "    \n",
    "    # Set the appropriate positions to 1\n",
    "    one_hot_labels[np.arange(N), random_labels] = 1.0\n",
    "    \n",
    "    return one_hot_labels\n",
    "\n",
    "def labels_to_hot(labels,num_classes=10):\n",
    "    N=labels.shape[0]\n",
    "    hot=np.zeros((N,num_classes),dtype=np.float32)\n",
    "    hot[np.arange(N),labels]=1.0\n",
    "    return hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d64cd-a7b0-4031-8825-488cc7e9818b",
   "metadata": {},
   "source": [
    "#### Check using random input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ef399-52fe-482b-a1e1-69bc9012f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(onnx_model, loss_fn, loss_fn_grad,\n",
    "                   param_name=\"fc1.bias\", batch_input=None, batch_labels=None, \n",
    "                   epsilon=1e-4, num_checks=5, gradients=None):\n",
    "    \"\"\"\n",
    "    Performs gradient checking for a specific parameter in onnx_model.\n",
    "    \"\"\"\n",
    "    # 1. Get forward and backward pass gradients\n",
    "\n",
    "    if gradients is None:\n",
    "        outputs = forward_pass(batch_input)\n",
    "        loss = loss_fn_grad(outputs[output_name], batch_labels)\n",
    "        gradients = backward_pass(loss)\n",
    "    else:\n",
    "        print(\"Using precomputed gradients.\")\n",
    "    \n",
    "    backprop_grad = gradients[param_name]  # shape: same as param in the model\n",
    "    \n",
    "    # 2. Flatten the parameter array\n",
    "    param_values = get_initializer(param_name)\n",
    "    flat_vals = param_values.flatten()\n",
    "    \n",
    "    # 3. Choose indices to check\n",
    "    idx_to_check = np.random.choice(range(flat_vals.size), \n",
    "                                    size=min(num_checks, flat_vals.size), \n",
    "                                    replace=False)\n",
    "    \n",
    "    # 4. Prepare array to store the finite difference gradient\n",
    "    finite_diff_grad = np.zeros_like(flat_vals)\n",
    "    \n",
    "    for i in idx_to_check:\n",
    "        old_val = flat_vals[i]\n",
    "        \n",
    "        # +epsilon\n",
    "        flat_vals[i] = old_val + epsilon\n",
    "        set_parameter(onnx_model, param_name, flat_vals.reshape(param_values.shape))\n",
    "        plus_output = forward_pass(batch_input)\n",
    "        plus_loss = loss_fn(plus_output[output_name], batch_labels)\n",
    "        \n",
    "        # -epsilon\n",
    "        flat_vals[i] = old_val - epsilon\n",
    "        set_parameter(onnx_model, param_name, flat_vals.reshape(param_values.shape))\n",
    "        minus_output = forward_pass(batch_input)\n",
    "        minus_loss = loss_fn(minus_output[output_name], batch_labels)\n",
    "        \n",
    "        # Restore original value\n",
    "        flat_vals[i] = old_val\n",
    "        set_parameter(onnx_model, param_name, flat_vals.reshape(param_values.shape))\n",
    "        \n",
    "        # Approx. gradient\n",
    "        finite_diff_grad[i] = (plus_loss - minus_loss) / (2 * epsilon)\n",
    "    \n",
    "    # 5. Compare with backprop gradient\n",
    "    # For simplicity, just compare the selected indices\n",
    "    backprop_flat = backprop_grad.flatten()\n",
    "    for i in idx_to_check:\n",
    "        g_bp = backprop_flat[i]\n",
    "        g_fd = finite_diff_grad[i]\n",
    "        rel_error = abs(g_bp - g_fd) / (abs(g_bp) + abs(g_fd) + 1e-12)\n",
    "        # print(f\"Index {i} => backprop = {g_bp:.6f}, finite_diff = {g_fd:.6f}, rel_error = {rel_error:.6e}\")\n",
    "\n",
    "    # (Optional) compute overall norm-based error\n",
    "    diff_norm = np.linalg.norm(backprop_flat[idx_to_check] - finite_diff_grad[idx_to_check])\n",
    "    sum_norm = np.linalg.norm(backprop_flat[idx_to_check]) + np.linalg.norm(finite_diff_grad[idx_to_check])\n",
    "    total_rel_error = diff_norm / (sum_norm + 1e-12)\n",
    "    print(f\"Overall relative error ({param_name}, for sampled indices) =\",total_rel_error*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0560290-0690-4bfe-b4aa-990015532d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model=onnx.load('normalized_model.onnx')\n",
    "# graph = onnx_model.graph\n",
    "\n",
    "def full_loss(input_data,labels_data):\n",
    "    global activations\n",
    "    activations = forward_pass(input_data)\n",
    "    output_data=activations[output_name]\n",
    "    # loss=l2loss(output_data,labels_data)\n",
    "    # dLoss_dOutput = l2loss_grad(output_data,labels_data)\n",
    "    loss=current_loss(output_data,labels_data)\n",
    "    dLoss_dOutput = current_loss_grad(output_data,labels_data)\n",
    "    gradients = backward_pass(dLoss_dOutput)\n",
    "    return loss, gradients\n",
    "# Generate random input\n",
    "output_name=onnx_model.graph.output[0].name\n",
    "input_name=onnx_model.graph.input[0].name\n",
    "\n",
    "# Perform forward pass\n",
    "#activations = forward_pass(input_data)\n",
    "#output_data=activations[output_name]\n",
    "#loss=l2loss(output_data,labels_data)\n",
    "# Perform backward pass\n",
    "#dLoss_dOutput = l2loss_grad(output_data,labels_data)\n",
    "#gradients = backward_pass(dLoss_dOutput)\n",
    "# Check gradient for all parameters of the model\n",
    "\n",
    "def full_gradient_check(input_data=None, labels_data=None, gradients=None):\n",
    "    for param in model_parms:\n",
    "        gradient_check(onnx_model, current_loss, current_loss_grad, param_name=param,epsilon=1e-3, \n",
    "                       batch_input=input_data, batch_labels=labels_data, num_checks=128, gradients=gradients)\n",
    "N=10\n",
    "input_data = np.random.rand(N, 1, 28, 28).astype(np.float32)\n",
    "labels_data = generate_random_one_hot_data(N)\n",
    "loss, gradients=full_loss(input_data,labels_data)\n",
    "model_parms=[]\n",
    "for param in gradients.keys():\n",
    "    if get_initializer(param) is None:\n",
    "        continue\n",
    "    model_parms.append(param)\n",
    "full_gradient_check(input_data,labels_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabdbd7-fe3c-4934-89b5-477381cc6546",
   "metadata": {},
   "source": [
    "#### Check using MNIST input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb5033-7c9d-437d-a303-953d1c02d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_model(onnx_model)\n",
    "param=\"fc2.bias\"\n",
    "orig=get_initializer(param)\n",
    "# print(orig)\n",
    "from itertools import islice\n",
    "# onnx_model=onnx.load('normalized_model.onnx')\n",
    "for images, labels in islice(train_loader,1):\n",
    "    images=images.numpy()\n",
    "    labels=labels.numpy()\n",
    "    hotlabels=labels_to_hot(labels)\n",
    "    loss,gradients=full_loss(images,hotlabels)\n",
    "    full_gradient_check(images,hotlabels,gradients=gradients)\n",
    "# print(gradients[param][1])\n",
    "orig=get_initializer(param)\n",
    "# print(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e830088-41b2-45ab-ab60-3bbebff229ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new=orig.copy()\n",
    "print(orig)\n",
    "ind=1\n",
    "set_parameter(onnx_model,param,orig)\n",
    "loss,gradients=full_loss(images,hotlabels)\n",
    "print(gradients[param][ind])\n",
    "print(loss)\n",
    "print(new.shape)\n",
    "new[ind]=orig[ind]+1\n",
    "set_parameter(onnx_model,param,new)\n",
    "loss,gradients=full_loss(images,hotlabels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54dfd8-7ea8-4f86-93ff-4ec85d923528",
   "metadata": {},
   "source": [
    "## Poor man's minimization: Gradient descent for l2\n",
    "We start by defining initialization functions for our model.\n",
    "\n",
    "Note: This is not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88262d9c-372a-4f00-b803-517f54cef215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_model(onnx_model):\n",
    "    for param_name in model_parms:\n",
    "        param_values=get_initializer(param_name)\n",
    "        set_parameter(onnx_model, param_name, np.zeros(param_values.shape,dtype=np.float32))\n",
    "def random_model(onnx_model):\n",
    "    for param_name in model_parms:\n",
    "        param_values=get_initializer(param_name)\n",
    "        set_parameter(onnx_model, param_name, np.random.rand(*param_values.shape).astype(np.float32))\n",
    "zero_model(onnx_model)\n",
    "random_model(onnx_model)\n",
    "orig=get_initializer('fc1.bias')\n",
    "print(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ece648-2978-4fe1-94a8-5580aca6e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "learning_rate=10\n",
    "epochs=10\n",
    "random_model(onnx_model)\n",
    "batch_size=100\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    for images,labels in train_loader:\n",
    "        count+=1\n",
    "        if (count==1):\n",
    "            images_np=images.numpy()+2\n",
    "            labels_np=labels.numpy()\n",
    "        hotlabels=labels_to_hot(labels_np)\n",
    "        for parm in model_parms:\n",
    "            parm=\"fc1.bias\"\n",
    "            loss,gradient=full_loss(images_np,hotlabels)\n",
    "            A=get_initializer(parm)\n",
    "            A=A-learning_rate*gradient[parm]\n",
    "            set_parameter(onnx_model,parm,A)\n",
    "            loss1,gradient1=full_loss(images_np,hotlabels)\n",
    "            print(np.linalg.norm(gradient[parm]))\n",
    "            print(loss,loss1)\n",
    "            #print(activations['8'])\n",
    "            break\n",
    "        if (count%50==0):\n",
    "            print(count,loss)\n",
    "            #print(activations['8'])\n",
    "        if (count==2):\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6088c-9a32-4188-b575-36bcfe7890fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(activations.keys())\n",
    "print(activations['onnx::Flatten_0'])\n",
    "print(activations['8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0a744-314c-479c-8f27-44aa30ad82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show imageness pruned weights\n",
    "weights = extract_weights(onnx_model,\"fc1.weight\")\n",
    "# Define MNIST-specific parameters\n",
    "weights=weights.T\n",
    "print(weights.shape)\n",
    "input_size = weights.shape[1]  # Flattened size of the input\n",
    "num_classes = weights.shape[0]  # Number of output classes\n",
    "\n",
    "# Visualize weights as prototype images\n",
    "visualize_weights(weights, input_size, num_classes, images_per_row=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6649de-4834-43eb-8600-32835eff4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_adam(model_params):\n",
    "    \"\"\"\n",
    "    Given a list of parameter names (model_params), return dictionaries\n",
    "    to store the Adam 'm' and 'v' states for each parameter.\n",
    "    \"\"\"\n",
    "    m_dict = {}\n",
    "    v_dict = {}\n",
    "    for param_name in model_params:\n",
    "        # Assume get_initializer returns the numpy array for that param\n",
    "        A = get_initializer(param_name)\n",
    "        m_dict[param_name] = np.zeros_like(A)\n",
    "        v_dict[param_name] = np.zeros_like(A)\n",
    "    return m_dict, v_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c0f59-0a56-4e81-b2f7-63fcbbfb6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_update_step(param_name, grad, m_dict, v_dict, t, \n",
    "                     beta1=0.9, beta2=0.999, eps=1e-8, lr=0.001):\n",
    "    \"\"\"\n",
    "    Perform one Adam update for the parameter 'param_name'.\n",
    "\n",
    "    Args:\n",
    "        param_name (str): Name of the parameter to update.\n",
    "        grad (np.ndarray): Current gradient for this parameter.\n",
    "        m_dict (dict): Dictionary holding the 'm' states for each param.\n",
    "        v_dict (dict): Dictionary holding the 'v' states for each param.\n",
    "        t (int): Current iteration number (1-based).\n",
    "        beta1 (float): Decay rate for the first moment.\n",
    "        beta2 (float): Decay rate for the second moment.\n",
    "        eps (float): Term to prevent division by zero.\n",
    "        lr (float): The learning rate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The updated parameter array (after one Adam step).\n",
    "    \"\"\"\n",
    "    # Retrieve old m and v\n",
    "    m = m_dict[param_name]\n",
    "    v = v_dict[param_name]\n",
    "    \n",
    "    # Update m, v\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "    \n",
    "    # Bias corrections\n",
    "    m_hat = m / (1 - beta1 ** t)\n",
    "    v_hat = v / (1 - beta2 ** t)\n",
    "    \n",
    "    # Load current parameter\n",
    "    theta = get_initializer(param_name)\n",
    "    \n",
    "    # Adam update\n",
    "    theta = theta - lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "    \n",
    "    # Store updated m, v\n",
    "    m_dict[param_name] = m\n",
    "    v_dict[param_name] = v\n",
    "\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3cde9-a127-4889-a28f-a6466e9ad038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Some hyperparameters for Adam\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "eps = 1e-8\n",
    "learning_rate = 0.001   # or 0.001, typically\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# 1) Build your data loader\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 2) Initialize your model parameters once\n",
    "# random_model(onnx_model)\n",
    "\n",
    "# 3) Initialize Adam state\n",
    "m_dict, v_dict = init_adam(model_parms)\n",
    "\n",
    "t = 0   # iteration counter across all epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        t += 1  # increment iteration counter\n",
    "        \n",
    "        # Convert Tensors to Numpy if necessary\n",
    "\n",
    "        #if (t==1):\n",
    "        images_py = images.numpy()\n",
    "        labels_py = labels.numpy()\n",
    "        \n",
    "        # Convert labels to one-hot\n",
    "        hotlabels = labels_to_hot(labels_py)\n",
    "        \n",
    "        # Forward pass & Gradient\n",
    "        loss, gradient = full_loss(images, hotlabels)\n",
    "        \n",
    "        # Adam update for each parameter in model_parms\n",
    "        for parm in model_parms:\n",
    "            updated_param = adam_update_step(\n",
    "                param_name=parm,\n",
    "                grad=gradient[parm],\n",
    "                m_dict=m_dict,\n",
    "                v_dict=v_dict,\n",
    "                t=t,\n",
    "                beta1=beta1,\n",
    "                beta2=beta2,\n",
    "                eps=eps,\n",
    "                lr=learning_rate\n",
    "            )\n",
    "            # Store the updated parameter back\n",
    "            set_parameter(onnx_model, parm, updated_param)\n",
    "        \n",
    "        # Print every so often\n",
    "        if (t % 50) == 0:\n",
    "            print(f\"Iteration {t}, Loss = {loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736fa3a-7d16-40b9-8b72-6830867c6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d88a7-c6df-4366-b170-e14449b8f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(activations['/fc1/Gemm_output_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0db245-b5bf-4ba9-9e6d-4343326f3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(activations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cbc8dd-c5ba-44a7-9cfc-9912b1d61f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
