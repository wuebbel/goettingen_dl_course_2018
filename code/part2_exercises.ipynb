{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Part 2\n",
    "\n",
    "These small exercises are intended to build on the knowledge that you acquired in the [classify MNIST](part2_classification) notebook. The exercises do not depend on each other. Feel free to pick those you like and do them in any order.\n",
    "\n",
    "We do not indend for all attendants to solve all of these during the course, but recommend that you at least read them.\n",
    "\n",
    "### General note\n",
    "\n",
    "If you have trouble understanding the explanations, or if there is an error, please [let us know](https://github.com/odlgroup/odl/issues). These examples are meant to be understandable pretty much without prior knowledge, and we appreciate any feedback on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Residual networks\n",
    "\n",
    "Residual networks were introduced in *Deep Residual Learning for Image Recognition*, He et. al. 2015 [arXiv](https://arxiv.org/abs/1512.03385). In residual networks instead of using the Multi-Layer Perceptron (MLP) structure\n",
    "\n",
    "$$\n",
    "x_{n+1} = \\rho(W_nx_n + b_n)\n",
    "$$\n",
    "\n",
    "We use a residual structure\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n + W_n^{(2)}\\rho(W_n^{(1)}x_n + b_n^{(1)}) + b_n^{(2)}\n",
    "$$\n",
    "\n",
    "In tensorflow, such a residual unit could be written as\n",
    "\n",
    "```\n",
    "tmp = tf.contrib.layers.fully_connected(x, n1)\n",
    "x = x + tf.contrib.layers.fully_connected(tmp, n2, \n",
    "                                          activation_fn=None)\n",
    "```\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Implement a residual version of the MLP. Does this allow trainig a deeper network?\n",
    "* Implement a residual convolutional network.\n",
    "* Using a small `n1` is called a bottleneck. How does the choice of `n1` affect the performance of the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Data augmentation\n",
    "\n",
    "In all of the earlier examples, we have assume that the training data is finite, but there are several invariances that we expect from our data, for example an image of a certain digit is also an image of the same digit if the image is rotated slightly or if some slight noise is added to the image.\n",
    "\n",
    "In data augmentation, we extend our dataset by adding these to the training data, and there is strong support for this in e.g. tensorflow.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Use [`tf.contrib.image.rotate`](https://www.tensorflow.org/api_docs/python/tf/contrib/image/rotate) and/or  [`tf.contrib.image.translate`](https://www.tensorflow.org/api_docs/python/tf/contrib/image/translate) to implement data augmentation. How does the results change? Does this allow you to train larger networks?\n",
    "* Use the function [`tf.random_normal`](https://www.tensorflow.org/api_docs/python/tf/random_normal) in order to implement data augmentation by adding noise. What should this do to your method? Is it equivalent to some other form of regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Training\n",
    "\n",
    "In the earlier examples we either used plain batch stochastic gradient descent (manually) or the ADAM optimizer [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) with a fixed learning rate (step size), but these choices are not set in stone, and the training speed and result of a neural network may in fact depend rather strongly on the training used!\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Train the networks using other optimizers such as [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer), [`tf.train.AdagradOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer) or [`tf.train.RMSPropOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer). How does the convergence rate change? Does the end-result change? Why?\n",
    "* The default learning rate is typically $\\approx 10^{-4}$. Try different learning rates by supplying the `learning_rate` parameter to the optimizer, e.g. `tf.train.AdamOptimizer(learning_rate=1e-1)`. How does this change the training speed? What about the final result?\n",
    "* By creating a placeholder and using it as a learning rate, the learning rate can be varied:\n",
    "```\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "...\n",
    "session.run(optimizer, feed_dict={images: train_images, \n",
    "                                    true_labels: train_labels,\n",
    "                                    learning_rate: ???})\n",
    "```\n",
    "What happens if you start with a high learning rate and then decrease it over time?\n",
    "* Most optimizers also have other hyperparameters, look at the documentation to find all of them. In particular, `AdamOptimizer` has defaults `beta1=0.9`, `beta2=0.999` and `epsilon=1e-08`. What happens if you change these parameters, e.g. set `beta2=0.99`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Hyperparameter tuning\n",
    "\n",
    "A hyperparameter is a parameter of the parameters. Examples include the size of the network, the nonlinearities used, etc.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* In the MLP we used `num_outputs=128` and `num_outputs=32` in the first and second layer respectively. What happens if we change these numbers, e.g. use `num_outputs=1024`?\n",
    "* For the MLP, what happens if we make the network deeper? What is the best depth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Nonlinearities\n",
    "\n",
    "The default activation function is the [`tf.nn.relu`](https://www.tensorflow.org/api_docs/python/tf/nn/relu) function, however, Tensorflow has several other activation functions.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Play around with e.g. e.g. [`tf.sigmoid`](https://www.tensorflow.org/api_docs/python/tf/sigmoid) and [`tf.nn.elu`](https://www.tensorflow.org/api_docs/python/tf/nn/elu). How do the results change?\n",
    "* You can even wrap your own function, e.g. `activation_fn=lambda x: tf.cos(5 * x)`. What properties should a good nonlinearity have?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
