{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learned Reconstruction\n",
    "\n",
    "In inverse problems, we want to recover some signal $x$ from noisy measurements $y$ where we know the forward model $\\mathcal{T} : \\mathcal{X} \\to \\mathcal{Y}$\n",
    "$$\n",
    "    y = \\mathcal{T}(x) + e\n",
    "$$\n",
    "In machine learning for inverse problems we aim to find some operator $\\mathcal{T}_\\theta^\\dagger : \\mathcal{Y} \\to \\mathcal{X}$ that inverts this process\n",
    "$$\n",
    "    \\mathcal{T}_\\theta^\\dagger(y) \\approx x\n",
    "$$\n",
    "\n",
    "This notebook demonstrates how to use ODL to perform learned reconstruction of the famous MNIST dataset. We demonstrate three ways of doing this\n",
    "\n",
    "* Fully learned reconstruction\n",
    "* Learned post-processing\n",
    "* Learned iterative reconstruction\n",
    "\n",
    "and we also compare to FBP-based reconstruction.\n",
    "\n",
    "The results should be approximately\n",
    "\n",
    "| Method                 |  Mean Error  |\n",
    "|------------------------|----------|\n",
    "| FBP                    | 0.01694  |\n",
    "| Fully learned          | 0.00226  |\n",
    "| FBP + learned denoiser | 0.00260  |\n",
    "| Learned Iterative      | 0.005150 |\n",
    "\n",
    "Note that the examples in this notebook may take some time to run, approximately 10-15 minutes each can be expected on a reasonable laptop.\n",
    "\n",
    "**Note:** This is a pytorch port of [the notebook using tensorflow](part3_learned_reconstruction.ipynb) with reduced text, see the other notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import odl\n",
    "from odl.contrib.torch import OperatorModule\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "torch.manual_seed(123);  # reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training and test data\n",
    "\n",
    "We create loaders for the MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load training and test data (from the official MNIST example,\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py)\n",
    "trafo = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "dset_train = datasets.MNIST('./data', train=True, download=True, transform=trafo)\n",
    "train_loader = torch.utils.data.DataLoader(dset_train, batch_size=50, shuffle=True)\n",
    "\n",
    "dset_test = datasets.MNIST('./data', train=False, transform=trafo)\n",
    "test_loader = torch.utils.data.DataLoader(dset_test, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ODL objects\n",
    "\n",
    "We initialize the ray transform on a space suitable for the MNIST images. We also create a filtered backprojection (FBP) operator for comparison, and for the FBP denoising net later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "space = odl.uniform_discr([-14, -14], [14, 14], [28, 28], dtype='float32')\n",
    "geometry = odl.tomo.parallel_beam_geometry(space, num_angles=5)\n",
    "fwd_op = odl.tomo.RayTransform(space, geometry)\n",
    "fbp_op = odl.tomo.fbp_op(fwd_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make pytorch `Module`s from ODL operators\n",
    "\n",
    "For use in our neural networks, we create pytorch modules from the above operators. For this we use the `OperatorAsModule` class from `odl.contrib.torch`. It implements automatic differentiation as required for backpropagation in pytorch, and it supports extra batch and channel axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fwd_op_mod = OperatorModule(fwd_op)\n",
    "fwd_op_adj_mod = OperatorModule(fwd_op.adjoint)\n",
    "fbp_op_mod = OperatorModule(fbp_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "For the generation of noisy projection data and for visualization of the results we use the following helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_data(images):\n",
    "    \"\"\"Create noisy projection data from images.\n",
    "    \n",
    "    The data is generated according to ::\n",
    "        \n",
    "        data = fwd_op(images) + noise\n",
    "        \n",
    "    where ``noise`` is standard white noise.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images : `Variable`, shape ``(B, C, 28, 28)``\n",
    "        Input images for the data generation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data : `Variable`, shape ``(B, C, 5, 41)``\n",
    "        Projection data stack.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(123)\n",
    "    data = fwd_op_mod(images)\n",
    "    data += Variable(torch.randn(data.shape)).type_as(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def show_image_matrix(image_batches, titles=None, indices=None, **kwargs):\n",
    "    \"\"\"Visualize a 2D set of images arranged in a grid.\n",
    "\n",
    "    This function shows a 2D grid of images, where the i-th column\n",
    "    shows images from the i-th batch. The typical use case is to compare\n",
    "    results of different approaches with the same data, or to compare\n",
    "    against a ground truth.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_batches : sequence of `Tensor` or `Variable`\n",
    "        List containing batches of images that should be displayed.\n",
    "        Each tensor should have the same shape after squeezing, except\n",
    "        for the batch axis.\n",
    "    titles : sequence of str, optional\n",
    "        Titles for the colums in the plot. By default, titles are empty.\n",
    "    indices : sequence of int, optional\n",
    "        Object to select the subset of the images that should be shown.\n",
    "        The subsets are determined by slicing along the batch axis, i.e.,\n",
    "        as ``displayed = image_batch[indices]``. The default is to show\n",
    "        everything.\n",
    "    kwargs :\n",
    "        Further keyword arguments that are passed on to the Matplotlib\n",
    "        ``imshow`` function.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if indices is None:\n",
    "        displayed_batches = image_batches\n",
    "    else:\n",
    "        displayed_batches = [batch[indices] for batch in image_batches]\n",
    "\n",
    "    displayed_batches = [batch.data if isinstance(batch, Variable) else batch\n",
    "                         for batch in displayed_batches]\n",
    "\n",
    "    nrows = len(displayed_batches[0])\n",
    "    ncols = len(displayed_batches)\n",
    "\n",
    "    if titles is None:\n",
    "        titles = [''] * ncols\n",
    "\n",
    "    figsize = 2\n",
    "    fig, rows = plt.subplots(\n",
    "        nrows, ncols, sharex=True, sharey=True,\n",
    "        figsize=(ncols * figsize, figsize * nrows))\n",
    "\n",
    "    if nrows == 1:\n",
    "        rows = [rows]\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        if ncols == 1:\n",
    "            row = [row]\n",
    "        for name, batch, ax in zip(titles, displayed_batches, row):\n",
    "            if i == 0:\n",
    "                ax.set_title(name)\n",
    "            ax.imshow(batch[i].squeeze(), **kwargs)\n",
    "            ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test data\n",
    "\n",
    "We take one batch from the test dataset and generate the corresponding test projection data for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a batch of test images and generate test projection data\n",
    "for i, (images, _) in enumerate(test_loader):\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "test_images = Variable(images)\n",
    "test_data = generate_data(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FBP reconstruction\n",
    "\n",
    "First we make a simple FBP reconstruction and compare it to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fbp_recos = fbp_op_mod(test_data)\n",
    "print('Average error:', F.mse_loss(fbp_recos, test_images).data / len(test_images))\n",
    "\n",
    "# Display examples\n",
    "results = [test_images, fbp_recos]\n",
    "titles = ['Truth', 'FBP']\n",
    "show_image_matrix(results, titles, indices=slice(10, 20), clim=[0, 1], cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully learned reconstruction\n",
    "\n",
    "Now for the first neural network that learns how to reconstruct. We use a rather simple approach of fully connected layers at each level, gradually proceeding from size `5 * 41` to size `28 * 28`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FullRecoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullRecoNet, self).__init__()\n",
    "        self.lin1 = nn.Linear(5 * 41, 1024)\n",
    "        self.lin2 = nn.Linear(1024, 1024)\n",
    "        self.lin3 = nn.Linear(1024, 28 ** 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x.view(-1, 5 * 41)))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network and see how well it does. We run 1 epoch for each model to have a somewhat fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use these parameters to steer the training\n",
    "use_cuda = True\n",
    "learning_rate = 1e-3\n",
    "log_interval = 20\n",
    "epochs = 1\n",
    "full_net = FullRecoNet()\n",
    "loss_train = nn.MSELoss()\n",
    "loss_test = nn.MSELoss()\n",
    "\n",
    "if use_cuda:\n",
    "    full_net = full_net.cuda()\n",
    "    test_data = test_data.cuda()\n",
    "    test_images = test_images.cuda()\n",
    "\n",
    "optimizer = optim.Adam(full_net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    full_net.train()\n",
    "    for batch_idx, (images, _) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "        images = Variable(images)\n",
    "        projs = generate_data(images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = full_net(projs)\n",
    "        loss = loss_train(output, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBatch Loss: {:.6f}'\n",
    "                  ''.format(epoch, batch_idx * len(images),\n",
    "                            len(train_loader.dataset),\n",
    "                            100. * batch_idx / len(train_loader),\n",
    "                            loss.data))\n",
    "\n",
    "\n",
    "def test():\n",
    "    full_net.eval()\n",
    "    loss = loss_test(full_net(test_data), test_images) / len(test_images)\n",
    "    print('\\nTest set: Average loss: {:.6f}'.format(loss.data))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    clear_output()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing a sample from the results in comparison to FBP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_full = full_net(test_data)\n",
    "results = [test_images.cpu(), fbp_recos.cpu(), test_res_full.cpu()]\n",
    "titles = ['Truth', 'FBP', 'Full']\n",
    "show_image_matrix(results, titles, indices=slice(10, 20), clim=[0, 1], cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples look much better than FBP (unsurprisingly), but there are still some issues with noise, deformations etc.\n",
    "\n",
    "To also make an assessment about the number of trainable parameters and thus the scalability of the approach (the more parameters, the harder to train), the total number of parameters in the model can be checked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of parameters:',\n",
    "      sum(param.numel() for param in full_net.parameters()))\n",
    "print('Training data size:', 60000 * 28 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that for this tiny problem, the number of parameters is over 2 million. This is still a good deal away from the input data size (47 million), but overfitting is likely with this network. Therefore we explore sparser architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FBP denoising net\n",
    "\n",
    "This network operates entirely on the 28 x 28 sized image domain by taking an FBP reconstruction as input and then trying to learn how to remove the noise and the streak artefacts. It uses convolutions that have far fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this network and see what we get. Note that in the training loop, we now have to use `fbp_op_mod(projs)` instead of `projs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "learning_rate = 1e-3\n",
    "log_interval = 20\n",
    "epochs = 1\n",
    "denoise_net = DenoisingNet()\n",
    "loss_train = nn.MSELoss()\n",
    "loss_test = nn.MSELoss()\n",
    "\n",
    "if use_cuda:\n",
    "    denoise_net = denoise_net.cuda()\n",
    "    test_data = test_data.cuda()\n",
    "    test_images = test_images.cuda()\n",
    "\n",
    "optimizer = optim.Adam(denoise_net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    denoise_net.train()\n",
    "    for batch_idx, (images, _) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "        images = Variable(images)\n",
    "        projs = generate_data(images)\n",
    "        fbp_recos = fbp_op_mod(projs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = denoise_net(fbp_recos)\n",
    "        loss = loss_train(output, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBatch Loss: {:.6f}'\n",
    "                  ''.format(epoch, batch_idx * len(images),\n",
    "                            len(train_loader.dataset),\n",
    "                            100. * batch_idx / len(train_loader),\n",
    "                            loss.data))\n",
    "\n",
    "\n",
    "def test():\n",
    "    denoise_net.eval()\n",
    "    loss = (loss_test(denoise_net(fbp_op_mod(test_data)), test_images) /\n",
    "            len(test_images))\n",
    "    print('\\nTest set: Average loss: {:.6f}'.format(loss.data))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    clear_output()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we visualize the results and compare them to the earlier ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_denoise = denoise_net(fbp_op_mod(test_data))\n",
    "results = [test_images.cpu(), fbp_recos.cpu(), test_res_full.cpu(), test_res_denoise.cpu()]\n",
    "titles = ['Truth', 'FBP', 'FLR', 'Denoise']\n",
    "show_image_matrix(results, titles, indices=slice(10, 20), clim=[0, 1], cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images look as if denoising was effective, but there are issues with streaks not being removed and instead taken as mage features.\n",
    "\n",
    "For comparsion we check how many parameters this model has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of parameters:',\n",
    "      sum(param.numel() for param in denoise_net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has less than 10000 parameters and thus better chances to scale well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative net\n",
    "\n",
    "The last variant uses an unrolled iterative loop with a learned gradient update as its network architecture. Thus, the network tries to get the best reconstruction out of a budget of iterations, but learning how to best update the current guess given the gradient of the $L^2$ data fit.\n",
    "\n",
    "Since we use only 2 iterations here, it is very important to already start from a decent estimate. We later use the FBP reconstruction as start of the iterative loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeNet(nn.Module):\n",
    "    def __init__(self, niter, op, op_adj, init_op=None):\n",
    "        super(IterativeNet, self).__init__()\n",
    "        self.niter = niter\n",
    "        self.op = op\n",
    "        self.op_adj = op_adj\n",
    "        self.init_op = init_op\n",
    "\n",
    "        # Make iterative blocks\n",
    "        for it in range(niter):\n",
    "            block = []\n",
    "            block.append(nn.Conv2d(2, 32, 3, padding=1))\n",
    "            block.append(nn.Conv2d(32, 32, 3, padding=1))\n",
    "            block.append(nn.Conv2d(32, 1, 3, padding=1))\n",
    "            mod_block = nn.ModuleList(block)\n",
    "            setattr(self, 'mod_block_iter_{}'.format(it), mod_block)\n",
    "\n",
    "    def forward(self, y):\n",
    "        if self.init_op is None:\n",
    "            # Effectively start from zero (not counting to iterations)\n",
    "            cur = self.op_adj(y)\n",
    "        else:\n",
    "            cur = self.init_op(y)\n",
    "\n",
    "        for it in range(self.niter):\n",
    "\n",
    "            # Get block of modules for dx\n",
    "            mod_block = getattr(self, 'mod_block_iter_{}'.format(it))\n",
    "\n",
    "            # Set gradient of ||A(x) - y||^2\n",
    "            grad = self.op_adj(self.op(cur) - y)\n",
    "\n",
    "            # Combine the two with learable parameters\n",
    "            dx = torch.cat([cur, grad], dim=1)\n",
    "            for i, mod in enumerate(mod_block):\n",
    "                dx = mod(dx)\n",
    "                if i < len(mod_block) - 1:\n",
    "                    dx = F.relu(dx)\n",
    "\n",
    "            # Iteration update\n",
    "            cur = cur + dx\n",
    "\n",
    "        return cur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this one, too. It will take significantly longer since it is deeper and requires more computation in each forward and backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "learning_rate = 1e-3\n",
    "log_interval = 20\n",
    "epochs = 1\n",
    "iter_net = IterativeNet(niter=2, op=fwd_op_mod, op_adj=fwd_op_adj_mod,\n",
    "                        init_op=fbp_op_mod)\n",
    "loss_train = nn.MSELoss()\n",
    "loss_test = nn.MSELoss()\n",
    "\n",
    "if use_cuda:\n",
    "    iter_net = iter_net.cuda()\n",
    "    test_data = test_data.cuda()\n",
    "    test_images = test_images.cuda()\n",
    "\n",
    "optimizer = optim.Adam(iter_net.parameters(), lr=learning_rate)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.1)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    iter_net.train()\n",
    "    for batch_idx, (images, _) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "        images = Variable(images)\n",
    "        projs = generate_data(images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = iter_net(projs)\n",
    "        loss = loss_train(output, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBatch Loss: {:.6f}'\n",
    "                  ''.format(epoch, batch_idx * len(images),\n",
    "                            len(train_loader.dataset),\n",
    "                            100. * batch_idx / len(train_loader),\n",
    "                            loss.data))\n",
    "\n",
    "\n",
    "def test():\n",
    "    iter_net.eval()\n",
    "    loss = loss_test(iter_net(test_data), test_images) / len(test_images)\n",
    "    print('\\nTest set: Average loss: {:.6f}'.format(loss.data))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    clear_output()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the result with earlier network architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_iter = iter_net(test_data)\n",
    "results = [test_images.cpu(), fbp_recos.cpu(), test_res_full.cpu(), test_res_denoise.cpu(), test_res_iter.cpu()]\n",
    "titles = ['Truth', 'FBP', 'FLR', 'Denoise', 'Iter']\n",
    "show_image_matrix(results, titles, indices=slice(10, 20), clim=[0, 1], cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are a bit mixed, and the value of the loss function seems to be at a higher level than for the others. Presumably, this model would need more training.\n",
    "\n",
    "How many learnable parameters do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of parameters:',\n",
    "      sum(p.numel() for p in iter_net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
