{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification\n",
    "\n",
    "In this notebook we tackle the perhaps most well known problem in all of machine learning, classifying hand-written digits.\n",
    "\n",
    "The particular dataset we will use is the MNIST (Modified National Institute of Standards and Technology)\n",
    "The digits are 28x28 pixel images that look somewhat like this:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/2202312/32365318-b0ccc44a-c079-11e7-8fb1-6b1566c0bdc4.png)\n",
    "\n",
    "Each digit has been hand classified, e.g. for the above 9-7-0-9-0-...\n",
    "\n",
    "Our task is to teach a machine to perform this classification, i.e. we want to find a function $\\mathcal{T}_\\theta$ such that\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|$\\mathcal{T}_\\theta$(|<img align=\"center\" src=\"https://user-images.githubusercontent.com/2202312/33177374-b134e572-d062-11e7-87c7-0574c6f5bee9.png\" width=\"28\"/>|) = 4|\n",
    "\n",
    "**Note:** This notebook is a condensed version of the [tensorflow notebook](part2_classification.ipynb). For full details, check the explanations there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies\n",
    "\n",
    "This should run without errors if all dependencies are installed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "This variant of the exercise uses [pytorch](http://pytorch.org/), the main competitor to tensorflow in the field of deep learning. It is developed by Facebook.\n",
    "\n",
    "\n",
    "### Execution\n",
    "The main difference to tensorflow is the *define-by-run* principle. In contrast to tensorflow's indirect way of first running operations on placeholders and then feeding the placeholders with data, pytorch runs computations immediately when executing a command.\n",
    "\n",
    "This approach is much easier to grasp in the beginning since it looks much more like regular Python code. Furthermore, debugging is much simpler since parts of a network can be run separately for testing, which is not easily doable with tensorflow. There are possible downsides, most significantly the lack of opportunity for optimization of networks (merge nodes, simplify steps based on rules etc.). In theory, this should give tensorflow an advantage in execution speed. However, this advantage does not show in practice.\n",
    "\n",
    "### Batched computation\n",
    "This works like in tensorflow: the first axis is the batch axis.\n",
    "\n",
    "\n",
    "### Channels\n",
    "One notable difference between pytorch and tensorflow is the placement of the channel axis: In pytorch, the axes are `[BATCH, CHANNEL, ...]`, whereas tensorflow uses `[BATCH, ..., CHANNEL]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the MNIST data\n",
    "\n",
    "Pytorch has a nice library for getting and transforming data, called [`torchvision`](https://github.com/pytorch/vision). It can be used with `DataLoader` objects that are iterators, like so:\n",
    "\n",
    "    dataset = torchvision.datasets.MNIST(...)\n",
    "    data_loader = torch.util.data.DataLoader(dataset, batch_size=50)\n",
    "    for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "        # Do stuff with the images and labels.\n",
    "        # `images` is a (50, 1, 28, 28) FloatTensor, and\n",
    "        # `labels` a (50, 1, 1) LongTensor.\n",
    "\n",
    "We load the MNIST dataset and use some normalization transform as well as conversion to pytorch `Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data (from the official MNIST example,\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py)\n",
    "trafo = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "dset_train = datasets.MNIST('./data', train=True, download=True, transform=trafo)\n",
    "train_loader = torch.utils.data.DataLoader(dset_train, batch_size=50, shuffle=True)\n",
    "\n",
    "dset_test = datasets.MNIST('./data', train=False, transform=trafo)\n",
    "test_loader = torch.utils.data.DataLoader(dset_test, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing an example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_loader.dataset[0][0]\n",
    "plt.imshow(img.squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "We define an MLP in pytorch by making a new class `MLP` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lin1 = nn.Linear(28 * 28, 128)\n",
    "        self.lin2 = nn.Linear(128, 32)\n",
    "        self.lin3 = nn.Linear(32, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x.view(-1, 28 * 28)))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These is the recipe we follow:\n",
    "\n",
    "- Subclass `nn.Module` by writing `class MLP(nn.Module)`. This signals to pytorch that this is a neural network with parameters that should be optimized.\n",
    "\n",
    "- Write an `__init__` method that initializes a new class instance right after it is created. Creation and initialization both happen in one step when we later run `model = MLP()`.\n",
    "  In this `__init__` method,\n",
    "  - run the initializer of the parent class by invoking `super(MLP, self).__init__()`. This is a necessary step for pytorch's internal bookkeeping;\n",
    "  - register any layers that have parameters. This includes `Linear` (= fully connected) layers, convolutional layers etc. As activation we use the ReLU nonlinearity that has no parameters, so we can skip it here.\n",
    "    Our layers map from input size (28 * 28) to 128, then to 32, and then to 10 (the class probabilities).\n",
    "    \n",
    "- Implement the `forward` method for the forward pass of the network. Here are the steps one by one:\n",
    "  - Flatten the images and remove the channel axis. This happens by the `x.view(-1, 28 * 28)` expression, similar to\n",
    "    NumPy's `reshape` function. The `-1` stands for \"whatever remains\", which allows to not know the batch axis size explicitly at this point.\n",
    "  - Run the flattened images through the first linear layer and apply the ReLU afterwards. We take the function version of the activation, which is more convenient and clearer than to store an instance of the `nn.ReLU` class (which would be possible as well).\n",
    "  - Run through the second and third layers as well, the last layer without nonlinearity.\n",
    "  - Return the `log_softmax` of the result.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Now we can train the model by using some optimizer, here stochastic gradient descent. The number of `epoch`s tells how often the whole training dataset should be processed.\n",
    "\n",
    "The global constants can be used to influence the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "learning_rate = 1e-2\n",
    "log_interval = 200\n",
    "epochs = 10\n",
    "model = MLP()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0]  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    clear_output()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of MLP parameters:',\n",
    "      sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional network\n",
    "\n",
    "Now we train a convnet for the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=2)\n",
    "        # Two convolutions with kernel size 3 and stride 2 reduce\n",
    "        # the input from (28, 28) to (6, 6)\n",
    "        self.fc = nn.Linear(32 * 6 * 6, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 32 * 6 * 6)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "use_cuda = True\n",
    "learning_rate = 1e-2\n",
    "log_interval = 200\n",
    "epochs = 10\n",
    "model = ConvNet()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    clear_output()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of convnet parameters:',\n",
    "      sum(param.numel() for param in model.parameters()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
